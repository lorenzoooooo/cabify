{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FCyot8tnZUKH"
   },
   "source": [
    "Challenge 1\n",
    "\n",
    "Success measure criterion --> Impact of service to increase restaurant's customers\n",
    "\n",
    "Impact measure criterion:\n",
    "    1. Compare sales before vs after picture posted\n",
    "    3. Compare quality of home made pictures with pro pictures -- The gap has decreased due to improvements and availability of technology and free tutorials\n",
    "    4. Since COVID more people order online --> Easier to sell food and then less need of high quality presentation? Or in the oder way higher standards?\n",
    "    5. Evolution of maintenance cost for photografy service\n",
    "\n",
    "Implementation:\n",
    "    Global:\n",
    "        For each year from 2001 to 2023: Number of subs to the service / Number of subscription to just eat --> Regression for the next X years\n",
    "        Evolution of the overall deployment and maintenance cost of the service --> regression for the next X years\n",
    "    Sample of restaurants divided by geographical criterion (maybe target, type of food):\n",
    "        Ratings (stars) of the service   \n",
    "        Percentage of sell increase with respect to before\n",
    "    Sample of customers divided by geographical criterion, target, type of food:\n",
    "        Percentage of orders coming from restaurant using photo service\n",
    "        Importance of picture criterion with respect to cost, qaulity and service criterion\n",
    "\n",
    "Avoid Bias by:\n",
    "    Create balanced dataset -- Balance by geographical areas, by price range\n",
    "    Normalization\n",
    "    K-Folding??\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IvfA0s_QZUKJ"
   },
   "source": [
    "Punto 2: Semplice RNN con dataset etichettato.\n",
    "\n",
    "Etichettatura multipla --> Softmax come attivazione finale\n",
    "\n",
    "0: “Both are the same”\n",
    "1: “They differ”\n",
    "2: “I don’t know”\n",
    "\n",
    "Come gestire il fatto che lo stesso percorso possa essere etichettato in maniera differente da 2 o + persone? Devo trovare un criterio per scegliere quale etichetta tenere. (Qual'è l'etichetta con la percentuale maggiore sullo stesso percorso?)\n",
    "\n",
    "Attenzione real e estimated routes no hanno per forza la stessa lunghezza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4924,
     "status": "ok",
     "timestamp": 1735650612785,
     "user": {
      "displayName": "lorenzo giraldi",
      "userId": "10228983639254068753"
     },
     "user_tz": -60
    },
    "id": "H1PsKUB1ZUKJ",
    "outputId": "c6809a93-712e-4ca9-b5a7-c4cb294bfe90"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "%cd /content/drive/My\\ Drive/cabify\n",
    "# with open(\"github_token.txt\", \"r\") as f:\n",
    "#   token = f.read()\n",
    "#  ! git clone https://{token}@github.com/lorenzoooooo/cabify\n",
    "%cd cabify\n",
    "! git pull\n",
    "!pip install pytorch-ignite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 5062,
     "status": "ok",
     "timestamp": 1735650617840,
     "user": {
      "displayName": "lorenzo giraldi",
      "userId": "10228983639254068753"
     },
     "user_tz": -60
    },
    "id": "e965AdEGZUKK"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1735650617841,
     "user": {
      "displayName": "lorenzo giraldi",
      "userId": "10228983639254068753"
     },
     "user_tz": -60
    },
    "id": "RxTbtPxmZUKK"
   },
   "outputs": [],
   "source": [
    "# #Load json to dataframe\n",
    "# df = pd.read_json('challenge_dataset.json')\n",
    "# print(f\"df size:  {df.size}\")\n",
    "\n",
    "# #Print a sample of real and estimated route\n",
    "# i=df.iloc[3]\n",
    "# df_estimated = pd.DataFrame(i['estimated_route'], columns=['Lat','Long'])\n",
    "# # print(df_estimated.size)\n",
    "# df_real = pd.DataFrame(i['real_route'], columns=['Lat','Long'])\n",
    "# # print(df_real.size)\n",
    "# plt.plot(df_estimated['Lat'], df_estimated['Long'], color='b')\n",
    "# plt.plot(df_real['Lat'], df_real['Long'], color='r')\n",
    "# # plt.scatter(df_estimated['Lat'], df_estimated['Long'], color='b')\n",
    "# # plt.scatter(df_real['Lat'], df_real['Long'], color='r')\n",
    "# plt.title('Annotator: ' + str(i['annotator'])+'\\n'+'Annotation: '+str(i['annotation']))\n",
    "# plt.legend(['Estimated route','Real route'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fTBdxuldZUKK"
   },
   "source": [
    "reflexions:\n",
    "    1. Dataset samples have different sizes -- Input size is variable --> how to declare NN?\n",
    "    2. Are points related temporally o spatially? RNN need equal temporal dependencies between samples. Otherwise can use Graph neural network.\n",
    "\n",
    "Approccio con feed forward:\n",
    "Layer iniziale deve avere dimensione ingresso -- Basic MLP are not fit to take variable input size, either introduce padding or cut to fized size\n",
    "\n",
    "GNN/GCN approach: Can take variable graph sizes as input\n",
    "\n",
    "The point is that I don't have single graph with label, but couple of graphs. I don't want to learn how to recognize the \"ight\" graph, I want to recognize if the graph differs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BL3aASQxZUKK"
   },
   "source": [
    "Feed forward attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1735650617841,
     "user": {
      "displayName": "lorenzo giraldi",
      "userId": "10228983639254068753"
     },
     "user_tz": -60
    },
    "id": "3ZQvK8I4ZUKK"
   },
   "outputs": [],
   "source": [
    "# input_size=??\n",
    "# number_of_features=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1735650617841,
     "user": {
      "displayName": "lorenzo giraldi",
      "userId": "10228983639254068753"
     },
     "user_tz": -60
    },
    "id": "LqDXtEM0ZUKL"
   },
   "outputs": [],
   "source": [
    "# class Net(nn.Module):\n",
    "\n",
    "#     def __init__(self):\n",
    "#         super(Net, self).__init__()\n",
    "\n",
    "#         # an affine operation: y = Wx + b\n",
    "#         self.fc1 = nn.Linear(input_size, number_of_features )  # 5*5 from image dimension\n",
    "#         self.fc2 = nn.Linear(120, 84)\n",
    "#         self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "#     def forward(self, input):\n",
    "#         f1 = F.relu(self.fc1(input))\n",
    "#         # Fully connected layer F6: (N, 120) Tensor input,\n",
    "#         # and outputs a (N, 84) Tensor, it uses RELU activation function\n",
    "#         f2 = F.relu(self.fc2(f1))\n",
    "#         # Gaussian layer OUTPUT: (N, 84) Tensor input, and\n",
    "#         # outputs a (N, 10) Tensor\n",
    "#         output = self.fc3(f2)\n",
    "#         return output\n",
    "\n",
    "\n",
    "# net = Net()\n",
    "# print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "osblonFFZUKL"
   },
   "source": [
    "\n",
    "Possible alternative approach: Convolutional network --> Generate images of routes with associated labels and learn to recognize. 5000 images to generate -- Expensive approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A1l4VihcZUKL"
   },
   "source": [
    "Generate fixed size image dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1735650617842,
     "user": {
      "displayName": "lorenzo giraldi",
      "userId": "10228983639254068753"
     },
     "user_tz": -60
    },
    "id": "GdprCU8lZUKL"
   },
   "outputs": [],
   "source": [
    "# #Load dataset\n",
    "# df = pd.read_json('challenge_dataset.json')\n",
    "\n",
    "# # print(f'number of same routes annotations: {df[(df[\"annotation\"]==\"Both are the same\")].shape[0]}')\n",
    "# # print(f'number of different routes annotations: {df[(df[\"annotation\"]==\"They differ\")].shape[0]}')\n",
    "\n",
    "# #List containing final dataset composed of journey_id and annotation\n",
    "# rows_to_keep = []\n",
    "\n",
    "# #Check if same route has multiple rating and if the annotations are different choose the one who is more rated\n",
    "# iter = df['journey_id'].unique()\n",
    "# for i in iter:\n",
    "#     #Select all the rows regarding the same journey_id\n",
    "#     mask=(df['journey_id']==i)\n",
    "\n",
    "#     #If this journey_id is rated only once append the journey_id and annotation to list\n",
    "#     if (df[mask].shape[0]==1):\n",
    "#         rows_to_keep.append([df[mask]['journey_id'].values[0], df[mask]['annotation'].values[0],\n",
    "#                              df[mask]['estimated_route'].values[0], df[mask]['real_route'].values[0]])\n",
    "\n",
    "#     #Otherwise choose the more used annotation on this journey_id\n",
    "#     else:\n",
    "#         # Count the number of times an annotation appears\n",
    "#         x=df[mask]\n",
    "#         tmp=[]\n",
    "#         tmp.append(x[(x['annotation']==\"Both are the same\")].shape[0])\n",
    "#         tmp.append(x[(x['annotation']==\"They differ\")].shape[0])\n",
    "#         tmp.append(x[(x['annotation']==\"I don't know\")].shape[0])\n",
    "\n",
    "#         #Select the annotation with max occurences\n",
    "#         # selected_label=max(tmp)\n",
    "#         idx_selected_label = tmp.index(max(tmp))\n",
    "\n",
    "#         # Map the index to the corresponding label\n",
    "#         labels = [\"Both are the same\", \"They differ\", \"I don't know\"]\n",
    "#         selected_label_value = labels[idx_selected_label]\n",
    "\n",
    "#         # Filter rows that match the selected label and append to list\n",
    "#         tmp_list=x[x['annotation'] == selected_label_value].iloc[0]\n",
    "#         rows_to_keep.append([tmp_list.values[0], tmp_list.values[2],\n",
    "#                              tmp_list.values[3], tmp_list.values[4]])\n",
    "\n",
    "# labels_path='labels.csv'\n",
    "# final_df=pd.DataFrame(rows_to_keep, columns=['journey_id', 'annotation', 'estimated_route', 'real_route'])\n",
    "# final_df.to_csv(labels_path, columns=['journey_id', 'annotation'], index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1735650617842,
     "user": {
      "displayName": "lorenzo giraldi",
      "userId": "10228983639254068753"
     },
     "user_tz": -60
    },
    "id": "DPMEdLuFZUKL"
   },
   "outputs": [],
   "source": [
    "# # Create directory to save images\n",
    "# output_dir = 'route_images'\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# # Turn interactive plotting off to not show every figure\n",
    "# plt.ioff()\n",
    "\n",
    "# #Create database of unique routes images\n",
    "# for _, df_sample in final_df.iterrows():\n",
    "#     df_estimated = pd.DataFrame(df_sample['estimated_route'], columns=['Lat','Long'])\n",
    "#     df_real = pd.DataFrame(df_sample['real_route'], columns=['Lat','Long'])\n",
    "\n",
    "#     fig = plt.figure(frameon=False)\n",
    "#     # Set the figure size to be square (for example, 6x6 inches)\n",
    "#     fig.set_size_inches(6, 6)\n",
    "\n",
    "#     image_path = f'{output_dir}/{df_sample[\"journey_id\"]}.png'\n",
    "#     ax = plt.Axes(fig, [0., 0., 1., 1.])\n",
    "#     ax.set_axis_off()\n",
    "#     fig.add_axes(ax)\n",
    "\n",
    "#     plt.plot(df_estimated['Lat'], df_estimated['Long'], color='c')\n",
    "#     plt.plot(df_real['Lat'], df_real['Long'], color='r')\n",
    "#     # plt.savefig(image_path, dpi=300, bbox_inches='tight') # Dimension = 1920*1440 pixels\n",
    "#     plt.savefig(image_path, dpi=37, bbox_inches='tight')   # Dimension 224*224\n",
    "#     # plt.savefig(image_path, dpi=5, bbox_inches='tight')   # Dimension 32*32\n",
    "#     plt.close()\n",
    "\n",
    "# #Execution time - 6min39s for image Dimension = 1920*1440 pixels\n",
    "# #Execution time - 1min30s for image Dimension = 224*224 pixels\n",
    "# #Execution time - 31s for image Dimension = 32*32 pixels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rFb6k2I3ZUKL"
   },
   "source": [
    "Create dataset splitting in Train, Val and Test - Unbalanced\n",
    "----Run only once, oherwise it adds to the dataset previously created ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1735650617842,
     "user": {
      "displayName": "lorenzo giraldi",
      "userId": "10228983639254068753"
     },
     "user_tz": -60
    },
    "id": "3eCJNDAPZUKL"
   },
   "outputs": [],
   "source": [
    "# Paths\n",
    "source_dir = 'route_images'\n",
    "dataset_dir='dataset'\n",
    "val_dir = f'{dataset_dir}/Val/'\n",
    "train_dir = f'{dataset_dir}/Train/'\n",
    "test_dir = f'{dataset_dir}/Test/'\n",
    "output_dir = 'labels.csv'\n",
    "# labels_list=['same','different','idk']\n",
    "#For now i don't create the idk class\n",
    "# labels_list=['same','different']\n",
    "labels_list=['different','same']\n",
    "classes=tuple(labels_list)\n",
    "dir_list=[train_dir, test_dir, val_dir]\n",
    "\n",
    "# Do the following only if the dataset has not been already created. To create new just empty the dataset folder\n",
    "if len(os.listdir(dataset_dir)) == 0:\n",
    "    #### Split Dataset equally between Train and Validation #############################################################\n",
    "    for i in dir_list:\n",
    "        if not os.path.exists(i):\n",
    "            os.makedirs(i)\n",
    "        for j in labels_list:\n",
    "            path=f'{i}{j}'\n",
    "            print(path)\n",
    "            if not os.path.exists(path):\n",
    "                os.makedirs(path)\n",
    "\n",
    "    ## Import csv file containing labels and annotation and count the number of total elements\n",
    "    labels = pd.read_csv(output_dir)\n",
    "    num_files = len(labels)\n",
    "    print('Numero di files nella cartella: ', num_files)\n",
    "\n",
    "\n",
    "    # Create mask for each annotation\n",
    "    mask_same = (labels['annotation']=='Both are the same')\n",
    "    print(f'number of same elements: {len(labels[mask_same])}')\n",
    "    mask_different = (labels['annotation']=='They differ')\n",
    "    print(f'number of diff elements: {len(labels[mask_different])}')\n",
    "    mask_idk=(labels['annotation']==\"I don't know\")\n",
    "    print(f'number of idk elements: {len(labels[mask_idk])}')\n",
    "\n",
    "    #Divide between training, validation and test (70%,10%,20%) --- Not using label I don't know\n",
    "    #Create Training set\n",
    "    train_set=[]\n",
    "    x=labels[mask_same]['journey_id'].to_list()\n",
    "    y=labels[mask_different]['journey_id'].to_list()\n",
    "    train_set = random.sample(x, int(0.70*len(x))) + random.sample(y, int(0.70*len(y)))\n",
    "\n",
    "    #Select the remaining data and split it between test (2/3) and validation set (1/3)\n",
    "    # To apply the difference operator I hve to cast the data to set. I cast it back to list for random samples.\n",
    "    remaining_x_set = set(x).difference(train_set)\n",
    "    remaining_y_set = set(y).difference(train_set)\n",
    "    test_set = (random.sample(list(remaining_x_set), int(0.66*len(remaining_x_set))) +\n",
    "                random.sample(list(remaining_y_set), int(0.66*len(remaining_y_set))))\n",
    "    val_set = set(list(remaining_x_set) + list(remaining_y_set)).difference(test_set)\n",
    "\n",
    "    #Copy images in the respective folder\n",
    "    for i in labels.values:\n",
    "        idx=i[1]\n",
    "        files=i[0]\n",
    "\n",
    "        #Categorize data based on the labels\n",
    "        if idx == 'Both are the same':\n",
    "            folder=labels_list[0]\n",
    "        elif idx == 'They differ':\n",
    "            folder=labels_list[1]\n",
    "        # else:\n",
    "        #     folder=labels_list[2]\n",
    "\n",
    "        #Place data in the correct dataset spit (Train, Test, Validation)\n",
    "        if files in train_set:\n",
    "            shutil.copy(os.path.join(source_dir, files+'.png'), f'{train_dir}{folder}')\n",
    "        elif files in test_set:\n",
    "            shutil.copy(os.path.join(source_dir, files+'.png'), f'{test_dir}{folder}')\n",
    "        elif files in val_set:\n",
    "            shutil.copy(os.path.join(source_dir, files+'.png'), f'{val_dir}{folder}')\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tgdI1vvjZUKM"
   },
   "source": [
    "size: 32*32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1735650617842,
     "user": {
      "displayName": "lorenzo giraldi",
      "userId": "10228983639254068753"
     },
     "user_tz": -60
    },
    "id": "OGI-PTBLZUKM"
   },
   "outputs": [],
   "source": [
    "# class convNet32(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "#         self.pool = nn.MaxPool2d(2, 2)\n",
    "#         self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "\n",
    "#         # self.fc1 = nn.Linear(16 * 5 * 5, 120)  ## From the calculation on papaer it should be input dimension=16*10*10\n",
    "#         self.fc1 = nn.Linear(16 * 10 * 10, 120)\n",
    "#         self.fc2 = nn.Linear(120, 84)\n",
    "#         self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.pool(F.relu(self.conv1(x)))\n",
    "#         x = self.pool(F.relu(self.conv2(x)))\n",
    "#         x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = F.relu(self.fc2(x))\n",
    "#         x = self.fc3(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WjG-NZiyZUKM"
   },
   "source": [
    "size 224*224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3410,
     "status": "ok",
     "timestamp": 1735650696468,
     "user": {
      "displayName": "lorenzo giraldi",
      "userId": "10228983639254068753"
     },
     "user_tz": -60
    },
    "id": "8fzjahOTZUKM",
    "outputId": "964e4d96-b022-4253-ee93-346639ab2c99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleCNNconvNet224(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=200704, out_features=2048, bias=True)\n",
      "  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  (fc3): Linear(in_features=512, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class SimpleCNNconvNet224(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNNconvNet224, self).__init__()\n",
    "\n",
    "        # First convolution layer\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)  # Output: 224x224x64\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)      # Output: 112x112x64\n",
    "\n",
    "        # Second convolution layer\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)  # Output: 112x112x128\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)       # Output: 56x56x128\n",
    "\n",
    "        # Third convolution layer\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1) # Output: 56x56x256\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)       # Output: 28x28x256\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(256 * 28 * 28, 2048)   # Flattened input to FC layer\n",
    "        self.fc2 = nn.Linear(2048, 512)             # Second FC layer\n",
    "        self.fc3 = nn.Linear(512, 2)               # Output layer (2 classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass through first convolution and max pool\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        # print(f'dimensione di x fopo conv1: {x.size()}')\n",
    "        # Pass through second convolution and max pool\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        # print(f'dimensione di x fopo conv2: {x.size()}')\n",
    "        # Pass through third convolution and max pool\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool3(x)\n",
    "        # print(f'dimensione di x fopo conv3: {x.size()}')\n",
    "        # Flatten the output for the fully connected layers\n",
    "        # x = x.view(-1, 256 * 28 * 28)\n",
    "        x=torch.flatten(x,1)\n",
    "\n",
    "        # Pass through fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # print(f'dimensione di x fopo fc1: {x.size()}')\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # print(f'dimensione di x fopo fc2: {x.size()}')\n",
    "        x = self.fc3(x)\n",
    "        # print(f'dimensione di x fopo fc3: {x.size()}')\n",
    "        return x\n",
    "\n",
    "print(SimpleCNNconvNet224())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E5QbfvA0ZUKM"
   },
   "source": [
    "Delcare hyperparameters, choose optimizer, determine if it's possible to use cpu or gpu, and load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 997,
     "status": "ok",
     "timestamp": 1735650723444,
     "user": {
      "displayName": "lorenzo giraldi",
      "userId": "10228983639254068753"
     },
     "user_tz": -60
    },
    "id": "PLblgq2RZUKN",
    "outputId": "6f1f9e88-510e-49f6-a119-52476b6eaee6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/Train/\n",
      "\n",
      "Batch 1:\n",
      "Images shape:  torch.Size([4, 3, 224, 224])\n",
      "Batch 2:\n",
      "Images shape:  torch.Size([4, 3, 224, 224])\n",
      "Batch 3:\n",
      "Images shape:  torch.Size([4, 3, 224, 224])\n",
      "dataset/Val/\n",
      "\n",
      "Batch 1:\n",
      "Images shape:  torch.Size([4, 3, 224, 224])\n",
      "Batch 2:\n",
      "Images shape:  torch.Size([4, 3, 224, 224])\n",
      "Batch 3:\n",
      "Images shape:  torch.Size([4, 3, 224, 224])\n",
      "dataset/Test/\n",
      "\n",
      "Batch 1:\n",
      "Images shape:  torch.Size([4, 3, 224, 224])\n",
      "Batch 2:\n",
      "Images shape:  torch.Size([4, 3, 224, 224])\n",
      "Batch 3:\n",
      "Images shape:  torch.Size([4, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import Compose, ToTensor, Grayscale, CenterCrop\n",
    "import argparse\n",
    "import torch.nn.functional as F\n",
    "import torch.nn\n",
    "from torch.optim import Adam\n",
    "import torch\n",
    "from datetime import datetime\n",
    "\n",
    "# Function to transform image\n",
    "def create_datagen(data_dir, batch_size=8):\n",
    "    # transform = Compose([Grayscale(), ToTensor()])    # for bn images\n",
    "    transform = Compose([ToTensor(), CenterCrop([224,224]),\n",
    "                         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])   # input: [224*224]\n",
    "    dataset = ImageFolder(data_dir, transform=transform)\n",
    "    dataloader = DataLoader(dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=True,\n",
    "                            num_workers=2)\n",
    "    return dataloader\n",
    "\n",
    "def dataset_description(data_loader):\n",
    "    print(data_loader.dataset.root)\n",
    "    print()  # Add an empty line for better readability\n",
    "    for batch_idx, (images, _) in enumerate(data_loader):\n",
    "        print(f\"Batch {batch_idx+1}:\")\n",
    "        print(\"Images shape: \", images.size())  # Prints shape of the batch of images\n",
    "\n",
    "        # Optionally, you can break after printing a few batches\n",
    "        if batch_idx == 2:\n",
    "            break\n",
    "\n",
    "parameters = {'train_dir': train_dir,\n",
    "                'val_dir': val_dir,\n",
    "                'test_dir': test_dir,\n",
    "                'log_interval': 2,\n",
    "                'epochs': 10,\n",
    "                'train_batch_size': 4,\n",
    "                'val_batch_size': 4,\n",
    "                'test_batch_size': 4,\n",
    "                'log_dir': f'tensorboard/logs_{datetime.now().strftime(\"%d%m%Y_%H-%M\")}',\n",
    "                'save_graph': False,\n",
    "                'load_weight_path': None\n",
    "                }\n",
    "\n",
    "train_loader = create_datagen(parameters['train_dir'], parameters['train_batch_size'])\n",
    "dataset_description(train_loader)\n",
    "val_loader = create_datagen(parameters['val_dir'], parameters['val_batch_size'])\n",
    "dataset_description(val_loader)\n",
    "test_loader = create_datagen(parameters['test_dir'], parameters['test_batch_size'])\n",
    "dataset_description(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sXeQRAmaZUKN"
   },
   "source": [
    "Show single mini-batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "executionInfo": {
     "elapsed": 533,
     "status": "ok",
     "timestamp": 1735650726050,
     "user": {
      "displayName": "lorenzo giraldi",
      "userId": "10228983639254068753"
     },
     "user_tz": -60
    },
    "id": "uDjN0lslZUKN",
    "outputId": "dc168f4e-2db4-4e66-bc20-78afa1e3e6f4"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAACtCAYAAACa74THAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAQghJREFUeJzt3Xl4TGf/x/H3mZlksicispFI7PsaSyxFKcWjqFLlUVVFW9TSUmqnaNHS2tpqiy66aJ9qtZYqSksEsdQWOyEklsgq28zcvz/S5ictKiSZSeb7uq5cF3POzHwnZ+bkM/e5F00ppRBCCCGEsCE6axcghBBCCPF3ElCEEEIIYXMkoAghhBDC5khAEUIIIYTNkYAihBBCCJsjAUUIIYQQNkcCihBCCCFsjgQUIYQQQtgcCShCCCGEsDkSUIQQQghhc6waUBYvXkxISAhOTk40adKE3bt3W7McIYQQQtgIqwWUr776itGjRzNlyhT27dtH3bp16dChA1euXLFWSUIIIYSwEZq1Fgts0qQJjRo1YtGiRQBYLBaCgoIYPnw448aNu+t9LRYLly5dwt3dHU3TiqJcIYQQQjwgpRQpKSkEBgai0929jcRQRDXlkZWVRVRUFOPHj8+9TafT0a5dOyIiIv6xf2ZmJpmZmbn/j42NpUaNGkVSqxBCCCEK1oULFyhXrtxd97FKQLl27Rpmsxk/P788t/v5+REdHf2P/WfPns20adP+cXtERAQBAQGFVqewDRcvXmTXrl088cQT1i5FFIGdO3eiaRrh4eHWLkUUgdWrV9OsWTPKli1r7VJEIUtOTmbFihW8/fbbuLu7/+v+Vgko+TV+/HhGjx6d+//k5GSCgoIIDAwkODjYipWJomAymXBzcyM4OFgu6dmB6OhoNE2jfPny1i5FFDKlFG5ubvj5+cnxtgM3btzAyckJ4J7O5VYJKD4+Puj1euLj4/PcHh8fj7+//z/2NxqNGI3GoipPCCGEEFZmlVE8jo6ONGzYkM2bN+feZrFY2Lx5szTrCiGEEMJ6l3hGjx5N//79CQsLo3HjxixYsIC0tDQGDBhgrZKEEEIIYSOsFlCefPJJrl69yuTJk4mLi6NevXps2LDhHx1nC5tFKSx//lsH6KSPgxD3TSmF+W+36bm3683i3ymlUGYzmqah6XRg579Xi1IopdBumS3jr3+rP39H8t4rvqzaSXbYsGEMGzas0B7/TlO8pJrN7IiLo9LFixy9do1N585xKDSUzrVr83JQkIQUIe6DUop9qak8d/w4aeacmFLJ2Zkfa9dGPlEFwGzm8gcfsCcykoZpaQTOn4+ubNkSHVKUUmT98QeJ16+TbjSis1gwmM04mEwYzGZ+TUjg9JEj+N24gVNWFo7Z2QB4+fujLl+m2pw5+Bfxl15RcIrFKJ78sCjFwaQkysXE8NW5c5xKSSHtz17D7jdv4p+QQJ0jR8jOyOCMtzeVrl3jkcuX2Z+ezuS+faFLF0YFBWEowR96IQra4bQ03r90iX0pKbQvVYquPj4ATD13jkWxsQwtWxa9fKYeiMrKwvThh/w0ejTnT5/moUGDqNO2LbqRI8FQ4k7lAGy+cYPFu3bx2P79BLi4kG0wYNLpsOh01AgLo33Vqqhq1TDp9bk/GY6OGLy8iHzpJcwmk7VfgngAJe5dbVaKmT/9xGOrV9P86ad5dts2Ug4fBqBUcDAOffpAaChanToQEpJ7v2ZffcWySZN4RtMo1a0bzwUGWukVCFF8JJlMbE9MZOjJk5Q1GvmoalVquLrmbh8cGMjQEyfo5uND8J9fFMR9unGDqAoVGNe+PaW9vHjzl184P2cOHZXCccQIcHS0doUF7uurV/mjbl1m/Pe/1LrlffVvLJmZJLi7k+rsXIjVicJW4gKKXtNoWaMG3o6OVN+yBaOHBy6bN+d8eDUN7jS1bq9ehABvvP02MytV4hFvb8rLCVWI27IoRVJqKj+tWsWGixd54emnGVup0j+GBbb18qKqiwt9jh7lpzp18Cyh3/SLguWzzzjl5kZVFxc8HRx49ZFHmGEw4DZlCq07dUJfs6a1Syxw71Wpwt6UFKq5uOTrfum//UaIpyc+9zAZmLBdVl3NuDDoNI2hdeuyd9o0lsfEoGJiQK/P+bnbvP+ahtazJ01Gj+bZuXN5bvduYjIyiq5wIYqJbJOJPw4d4vMxY6i7aRNLV61izIkTt+0MW8rBgZeDgnDS61l77RoW6yz9VSIkOTsT2LIlVf/8Y+1pMDCpdWt+HzWKC2PHos6ft3KFBU+naTT28Mj3JffYy5dxLVWK0g4OhVSZKAolLqAAGHQ6nipfnk09enAmJgb274d7OTHqdOg6d6aLUnRctIgJhw/fsaOtEPZGKcXNpCS+mzuXy+PG0bVtW2p8+CHu7dtjmD37jp+xrj4+PB8YyOjTpzmZnl7EVZcMKj2d9OPHia5UKU9fHk8HB5q2acOHlSuTOXbsvZ3n7IDeYuGap6e1yxAPqEQGFICqLi6807Mnfp07w5NPwm0WIbwtNzccPvuMF5KSqLh8OZ9fvCghRdg9pRRbo6L4csIESt24QYuFCyn3+OPovbzg+echJgbWrr3j/Tt7e1PdxYV5Fy4UXdElSHpWFrtjYxkUGvqPbY+ULk3I0KF8YTZj+fhjCSmZmThHR6OqVbN2JeIBldiAAhDk4oL72LHQrx9s337vd/T3x2nlSkZeuMCh+fNZdeGCNE0Lu2SyWDh54QLfz52LNnkyD3XqRJNJk3CvWBFNr8/ZqW5deOcdmD0b7nCZwVmvZ2S5cmxISOCn69cl9OeTZjCgL12a2Nt80dJpGl3Kl+f3QYPYu3Qp6uefrVCh7chOS+NMZCTtJaAUeyU6oABgNML06fDqq/m6m+bvj+cHH/DqqVPsXbiQzy9exCwnVWFHTt64wTtffsnusWOpHhdH84kTqdixIx6363jYpg04OcHixWD++1RtObr5+DAkMJBJZ8+SZrHcdh9xe04uLrRq2ZLUb7/ldFzcP7b7OToyq00b1vTvT+L776OuXrVClbbBotNx0dcXk3TILvZKfkCBnNE79zEHg+bvT6kPPmDisWNsW7aMz2NjJaSIEi/bbOarffuImjiRTosX89jQoVSZOxfHZs3uPCunlxe8+CJ8/DHExt52F03T6OPrS7LJxPfXrkkrSj5omobHU09RuWpV1n33Hdm3CXh+jo5U7NqVZR4emN5/H+w0BOrS0khxciJTOsgWe/YRUB6A5u+P97JlvLFvH1s/+ojPL12Syz2ixFJKcTw6Gv1bb9G5cmWq/vQT7s2b///lnLtp3x6qV4f58+/4x7GCszN9/PyYExNDlnyO8kVzdCS4a1eqrlvHB4cO3TbgPRMURPygQURs3Ihl3TorVGl9Ge+9h2u5cuiNRmuXIh6QBJR7oAUEUHrZMubs3csPq1ZxKC3N2iUJUeCUUiRdvszFt96ieUgIbiNGoPPyuvfWRy8vGDIEvvwSzp27424TypenvJMTE86evW1LgLgzrX59WsTFcXLfvttu12saL9avz7dPP03M6tWolJQirtD64po2xfXQIU5PmcK5I0dy1i+SMFwsSUC5R1pAAD7vv8/0NWv4fMsWrv255oMQJYU6cYLTw4bhVqsWpSdOvL9F1rp3h/Llc1pR7sCo0zG6XDk+j48nXQJKvmjkLIan7nJsKrq4MKJ3bz719CRlxAhISiq6Am1AhUcfpcn8+VwtXZqLS5awedAgLq1cybXISDLOnrV2eSIfpBdRPmj+/lR/6ilaLVjAK15evBUeLhMBiRLBcuwY54YN4/sGDRj03HM43u8U4a6uMGJETqf0F1/MueTzN2lmM19dvUoZBwdZnyefFHCqbFmyg4Luul+wqyvuzz/PD2PG0OP333Hu3LloCrQBer0e/woV6Dh2LOnJySScOMHByEjMK1bgk51NVqlSZBkMpLq4cMPdnSRXVy4EBDC2bVv8ZWp8myIBJT90OrShQ+moacQtWMALTk58FhaG491mqBXCxllMJk6tXMnGChVoN3o05R50evAnnshpQTl3LiegnD+fM5Kubl144QWiMzJYGRfHxPLlcZbPTv7s2MEBT0/6NWhw1xYug07HS9Wr88Urr3Bw+nQau7qia9WqRK98fDvOHh6UDQsjsGFDAKLT0rhy+jQh16/jkpGBc1YWTomJbFmyBH3jxiABxaZIQMkvTUP3/PP8F0hbvJhtb7xBW39/dHb2wRclg8Vs5sS333Lw/HkenjaNGv7+93dp51YGA4wbB3PnQpUq8NRTOf1T1qyBhx6inrMz037+mVWtWjG8bFncZTjovVGKjFOnSGjenMZeXv+6u07TCGvcmCXduuE3cyYhTZui2en6Yn+9p6u7uVG9bt0827JTU7m5YQP6OwyPF9YjX1/uh16PY/fu9Dtxgq1vv83PV65IJyxR7CilOPrddxz53/9oPH48NSpXfvBwAjnf0jt3hsxMaNIEypaFzz6Dli1zZp3t3ZvWN28yv3ZtXO9ldJAAQGVmcu6LL/Bo0OCeL41VcXFh8DPP8HXTpqRPmIDKzCzkKoshTcNgsaCX/lA2RwLKfdICA/H65BOG7dnDj0uXckFG9ohiRCnF7vXrubBqFWGvvkpI7doFE07+4uiYM7Psxx/DihXg6wtDh8LRo+hr16bR9Om0lpbH/Dl5kp3+/jSpUuWe76JpGjU9Pcl66imWXbyIeexYlCyCmpdSVIyNxVV+LzZHAsoD0CpXxv/DDxly8CA7v/yS1Kwsa5ckxL8yWyxs/P134lesoP6YMZT/l/4M90XT4KGH4LHH4K9F29q1y1lyYtEicHMr2OezA5kbN3I6KAijo2O+7/ty1aqkjRpFRGQkHDxYCNUVX5/GxWFKTbV2GeI2JKA8IF2lSlRauBCfXbtY9tNPJJtM1i5JiLv64cwZzi1cSNMXX8Q/PLzonljToF498PAouucsKUwmkjIycG3Vior30ZHTRa/nuQYNmD9kCDEjRqBk0cZcTUqX5nCLFkS8+y5XJbzZFAkoBcCpbFlaPPkkZZcuJSo6WvqjCJuVmJCAw8KFPO7kRJl69axdjrhHlsREjm/ZQrPKle/7Mco4OPBKt24sbdmS9JUrQVp8Aajh5UWrqVPR1arFvhkziP9zcjdhfRJQCoCmaRjbtKFjmzYkzprF4bg4eYMLm5N8/To7ZsygvMVC6YUL0e5hJIiwDZbt24msVIlqvr73/RiaphHm6UlCnz58t2MHljlzQM5TaJpGSJkyhD/7LJ7du7N3+nR2bthApgQ4q5OAUkA0gwG3l1+mRYMGbH7zTfbKkvLCVihF+tWrRE2ahJurK1VmzUL/V78QUSwkREVhqVbtvvqf3MpRp+PtOnU489JLnPzlF9TZs4UeUkxKsSo+ngQbn31bp9cT9uSTVOrenZSvvuK3JUvIlo6zViUBpQBpjo74DB9Od7OZNYsWcUPe3MIGKODKuHGcNBppPG4cxgediE0UrdRUrt+4QenGjfEugJmr3fR62jdrxqLWrbkxYEBOSClE2WYz70ZGEp+eXqjPUxAMBgNVe/cmfMYMsg8dYu/8+Zy+w+rcovBJQClgmtFIuYED+c/+/Zz96CNMMu+AsCKlFBfWr2ebxULI88/j5Opq7ZJEPmVevcqFM2doU6NGgT1mE09POg0ZwueVKmFevhwKcZIyY1YWixYswD8hodCeo6B5BgURPmMG5vh4dkyaRPQff0iLuBVIQCkEurp1CV24kJi9e9n/4Ydky7VMYQXKYuHspk0cXLaMmi+8wCNVqhT8cGJR6JIyMjgRGkqAi0uBPu6j/v7EDhnCpm3bsHzxRYE+9q10jo6Etm5NWjGbK8ozIIDm8+fTvEMHTk2fzsmoKFLkXF6kJKAUAk3T8AsKosW0acRHRPDZ9u2SvkWRy/zhBw7On0/lESNo0KiRhJNiauvWrQTVq4fhAfuf/J2maYypX58tQ4Zw+auvUJcuFejj5zIYoHp1FhazlYQ1TUPTNCr07EmTTp2Iee01vli+nJs3b1q7NLshAaWQaJqGT3Aw9QYPxmPlSi5evSohRRSZ7MREojZv5mD37vg1aybhpBgrfeMGdXx8CmVR0tIODjRq04bFoaEkz52LKqTp3k16PZViYgrlsQubptNR5umnaf7xx1SLiuK3uXO5mZIi5/MiIAGlEGmahn+TJtSqXp1vP/4Yi6z1IIqA+cYNoidOZHeFCrzYty+lCvibtyg61zIzcbx6Fd8yZQrtOXoEBNB4+HAiTpzA/M03hfIcWQYDPsnJhfLYRcJgwLlcORq+/jqOcXF8O2IEUfv2SUgpZBJQCpnBaMT87LNoBw/CnDlg40PtRDF3/Tpxo0axJzCQ/w4ejK90ii3WEq5cIT41FZdmzQrtOfSaRscKFdjUrx/H/vc/LDduFPhzZDs44FwCBgy4+voSPnMmLbp149Ls2ZyUy/eFSgJKESjr48OZF1/k53XryJ43T2ZwFIVCKUXSK6/wkdFI+QEDKCPhpNi7mZBA0NWrhf48Rr2e0V278mOFCiS99BKqgEOKSafDWELOe07e3oR06UKtgQOJXriQjW+/TWpiorXLKpEkoBQBT4OBGeHh/D5pEus2bCDjrbckpIgCpZQi5ddfWa3XE/zyyzzs72/tkkQBWB8RQZVmzaAQ+p/8XaCTE14DBrA6PZ3UNWtQBXhJRlOqRK0WrGkaoY8+Sp3Jk3G+coXIqVO5WYyGURcXBmsXYC88DAbGPfwwM/R61Ouv075TJ1zq1rV2WaIEUEqRsm0bv8+fj8/kyTxWubJ0ii0JsrOpf+wYCb17410EAUXTNAZXqsSaMWNY9dZbdNyyBWPNmrhmZGDMzkbXqxdaaCgohQbc8R1mMMDfWu8CypXDUACTzNkSTdMIqVMHv9BQts6Zw67Jkwno2ZOqLVuiK4LjZQ8koBQhd4OBSa1a8cPp02x/7z0azJ5NGU9P+WMi7p9SpG/ezB+zZ+M+fjzNGjRAJ++nEiFr+3Ycz57FXKVKkT2nXtPo2qgRn86dy8fXruGbmEj5uDj2HjqE/uOP8cjOxicpCe/kZHTZ2ShNo9zVq7inpeGSmYlLZiYOlSpBr17olEIB6UYjpj/+4LqTE6FF9kqKjrO7O63GjuXAN99wcsEC9ImJVO7cGU2vz1nBW9w3CShFzN1g4Mk+fbgeFcWad96h49ixlHNykpAi7ovp4kUOT51K7NChPNGmDXr55lZipOl0XKpThzbe3kX6vAadjgHly0P58kBOC13tzEyyLBZ0FguOJhOa2cyCmBium0y4ZGRgMJsxmM3oLRZ0SpGRlMTlP/6gmdGId5kynHnkEXwqVSKsSF9J0XF1d6fZM89wslYt9r77Lnz5JYELFuDq6yvn9gcgAcUKDG5u+L75Jo/27cuSTz/l+X79KO/sbO2yRDGTffMmBz75hJ3t29PlP/9Br9dbuyRRQJTFQsayZVzr0cPapaBpGsFOTv+4fW7p0ne8T4rJxJEOHQh1csLPToa5a5pG5bAwLk+dys6PPqLs5Mk0nj4dTz8/a5dWbMnXLSvRvLwIevZZeixbxvS1a0k1maxdkihGMjMyODh/PnEXLtB72DAqygKAJc4fDg50qVu3WH4DdzcYaOrhYTfh5C+aptGqYkWemjgRY3AwkZMmcfXECcxyfr8vElCsSHv8ceqPH0//999n54EDmGUiN3EPsjIz2fzRRyScOUOLadPwK1XK2iWJAmbZuZNLmsaNu7RSCNtldHGhyejRaFWq8NuYMez/5hsJKfdBAoo1aRq6bt2o/8orxM2fz5roaMwy6Y+4C6UUZ7duxbJnD02nTcNLrnGXSDcuXcKtenXqe3lZuxRxn4zOzjw8ciRNZ87k8rffEj1nDuajR2Vit3yQgGJlmk6HW4cOdG7cmBNvvcWp69etXZKwVUpx8+BBzi1bhtanDx7lykk4KYFUdjZZBw4QVbMmejm+xZreYCCgZk1qT5/OtZs32TF1Kinnz0tIuUcSUGyAptPh/dRT9MrK4uiiRSSkp1u7JGGDLDducGbUKC527MgjDz9s7XJEIVHZ2eyPjqZvrVrWLkUUAE3TCKlenWaTJmFq25adkydzY9Mmtl68KH0P/4UEFBuh+foS9M471Dl3jreXLmXZpUtYJGWLP1kyMji5fDkHqlalSbduOBpkAF5JZfr6a674+OBZiAsEiqLnYDTS4plncH7oIX5ZuZLTQ4bw8/ffW7ssmyYBxYY4entTYc4cXli/ni0rV9L36FHOSmuKAJKOHyf6119pO24cNaXjZIl2NTWVgLAwglxcrF2KKGCORiMPDRxI3aVLadOoEeUXLsR8/Li1y7JZElBsjObrS+CTT7L07bfJ/uYbnjh0SEKKnbt54gQHZ83C1KcPvkFB0u+kBFPJydz8/XcO16wpx7mE0jSNqh4eZIwezcaWLflj0ybMssr9bUk7sQ3SBgzAy2zm4/HjeRbopdPRo0wZHvPxoYqLCwY5cdkNZbFwccYMImvWZNQTT2CQydhKNHNqKlEWC/1q1rR2KaKQ1XB3x3vgQPZNmIDFYKD+c8+hk0u3eeS7BWX79u106dKFwMBANE1jzZo1ebYrpZg8eTIBAQE4OzvTrl07Tp48mWefhIQE+vbti4eHB15eXgwcOJDU1NQHeiElil4PAwfiMXs2Hy1cSIvly9l68iSt9+9nwpkzZMt8KXZBWSzEL1tGlE7HI/374yAnrxLPtGgRybVrY/zbYnui5NE0jYCQEBpOm8b5iAgOfvghFuk0m0e+A0paWhp169Zl8eLFt90+Z84c3n33Xd577z0iIyNxdXWlQ4cOZNyy1Hbfvn05cuQImzZt4scff2T79u0MHjz4/l9FSWQwwHPP4Tl7NvN37ODbIUP4bNQovt+9m2WXL1u7OlHYTCZM8+ax7bvvqPjKKzQoX16a/Eu61FRi4+NxbdMGrxK28q+4M/9KlWgyeTJnIyI4uGwZ5qwsa5dkM/L9laxjx4507NjxttuUUixYsICJEyfStWtXAD755BP8/PxYs2YNvXv35tixY2zYsIE9e/YQFpazdNTChQvp1KkT8+bNIzAw8B+Pm5mZSWZmZu7/k5OT81t28aTXw6BB0LUrbgcP0v7KFb5esID5GRkceeIJasr05iWW6cIFdq1bx6FXX6VD9erWLkcUgaxLlzgWH08TGV5sd8pWrEiTqVPZN2kS6Y6ONOrXDwc7Wybgdgq0k+zZs2eJi4ujXbt2ubd5enrSpEkTIiIiAIiIiMDLyys3nAC0a9cOnU5HZGTkbR939uzZeHp65v4EBQUVZNm2z9cXHnkE+vSh9syZDPvwQ5YdPEiWXOopkdTZs8QMH86RAQMY264dXnJpxy5kffstB8PDKevmZu1ShBUEhoTQbOpU0jZsIPLzzzFJx9mCDShxcXEA+P1t9UY/P7/cbXFxcfj6+ubZbjAY8Pb2zt3n78aPH09SUlLuz4ULFwqy7OJD09DatqXOf/6D45dfsiAmBpPMlVKiqLg44p97jsX16/N4z554SFO/fTCZOB8TQ3jbtjhJR2i7pGka3hUr0mDWrJyQ8tlndr9+T7EYZmw0GvHw8MjzY7c0DcOgQQzdsYPPfvuNXfZyucsOKIuFhM2b+SIkhDbPP4+Ps7O1SxJFRO3ZQ8zhw1woW1b6GtkxTdPwrlSJBjNmkPTzz0R8/jlms9naZVlNgQYUf39/AOLj4/PcHh8fn7vN39+fK1eu5NluMplISEjI3UfcnebtTfD48cxbvpw39+3jYmamrO1Q3ClF1iefEPHNNzQcO5ZOgYHo5A+V3cjUNM6HhdFVzoF2T9M0ylSpQqMpU0jasIGdq1bZbUtKgQaU0NBQ/P392bx5c+5tycnJREZGEh4eDkB4eDiJiYlERUXl7rNlyxYsFgtNmjQpyHJKLp0O7YkneMTVlcfnzKHf/v1cvKUTsSh+TL/+yvbly3EaN46WVapIOLEjSiluLltGUsOGeEh/I/GnMtWq0XjyZBI3bGDHqlV22Scl3wElNTWVAwcOcODAASCnY+yBAweIiYlB0zRGjhzJ66+/zg8//MChQ4d4+umnCQwMpFu3bgBUr16dRx99lEGDBrF792527NjBsGHD6N27921H8Ig70OnQPvyQXjodwWvW0PPIEc7fMpRbFBNmM6boaPasWcPvgwZRu359aeK3Q8eAhxs1kmAq8ihTvTrhkyeTvGEDMe++C3Y2q3i+A8revXupX78+9evXB2D06NHUr1+fyZMnAzB27FiGDx/O4MGDadSoEampqWzYsAEnJ6fcx/j888+pVq0abdu2pVOnTrRo0YIPPviggF6SHfHzw3XaNN756CP816+n5+HDRKelyURuxYXJhHnFCiL79WN7zZqMePxx/GRood1R+/cTk51Noo+PtUsRNqh0lSqol19m98aNpM2cCXbUWp7v9sTWrVvftb+DpmlMnz6d6dOn33Efb29vVq1ald+nFrdTvz5e06axYtIkBjg4EJ6RQW9fX4aVLUsVZ2ccdMWiH7TdUWYz5pUr+WXlSn6YMIGpHTvibTRauyxR1JQi+eJFzKGhtPH2tnY1wgZpmsYj9eqxYPx4Ns6YQYfZs3GZMsUuWlrlr1dxp9fD4MF4devGp9u28UVwMOczMmh94AAvnz7Nirg4bprN0onWhiilyP70U3746is+nTSJKR074ivhxC4ps5nUxYvRhYfLGlvijpz0ekY89BAXXnmFn0+cIO3CBbs4p0tAKQkMBnj5Zdx27eLR48f5umZNttWrh6fBwKzz5wmLiuKT+HgS7bQnuC1RSpGwejW/fvstURMm8HarVvhJOLFf58/zh4sLDWrXtnYlwsY56/U81749yd268ev06aTGxFi7pEInAaWkqFQJRo2CJUtwy8ighqsrM0JD2VCnDn39/Jh5/jyDjh+XkGJFFqXYs2MHEV99RZmpU3n9oYekz4mdM23axKnSpTF5elq7FFEMuBoM9OrWjfRWrfh96lRSzp61dkmFSgJKSeHoCGPGwLlzcMuSARWcnXktOJhf6tYlJiODYSdO2EXToK0xK8WaEyeIfv996r34IvUaNLCLa8jiLiwWUm/exNS2LTVl9WJxj5wdHenSuzdJ7dqxc8oUUs+csXZJhUYCSkliNOaElLlz4ZYVMTVNI8ho5M2KFdmSmMjulBQrFmmfUhITOT1/Pu0DAij78MMSTgQqLY1Lq1dTq25deT+IfHFycKB7r17c6NCByEmTSDt92tolFQoJKCWJpkGPHnD1KsyaBbe0lGiaRgtPT+q4unIkLU1aUYqKxYL5yhXOvvYa9Vxc8B47Vv4YCQDUrl3sCgmhatmy1i5FFEPGP0PK9UcfZf+ECaSfOVPizusSUEoaR0d45RVYuRLOns0TUgyaxvjy5Vlw8SKZJeyNbIuUUlxeu5a9vXuzrUYNGk2bhqPMdSH+lLljBymVKmG8ZY4oIfLD6ODAY717E/fooxwaO5asc+dKVEiRgFLSaBp06gQtWuS0pvzt+mQ5o5EUs5nP/rZekihYSimObtnC3k8+IWLYMPoMHoyXu7u1yxK2IiODG9ev49CyJf4yiks8ACcHBzr36cPFzp05NnIkplOnrF1SgZGAUhJ5eMDixVCzZk5YmT4doqIgLY2Kzs48WaYMR9LSMJWgpG1LlMXCpbVrObtkCTWHDWNE9+4yz4nIQ127xrnduzEHBVm7FFECODs60rFvX8527crpl17CdPy4tUsqEBJQSioPD1i6FKZNg7g46NgRnngCNm3iBaX4LC6Oc7J2T4FTZjNXfviBgx9+SPmxYwlt3Vr6nIh/MCclsathQ/4bEmLtUkQJ4ezoSKd+/TjeowfHR40i88QJa5f0wCSglGTu7tC7N7z7LmzeDA8/DBMnUqptW5r+9hvzY2IwSytKgVFmM9e/+459y5YROHEitRo3lnAibuvme+/hVacOTjIPjihARgcHOvbvz4X//IezI0ZgLuYhRQKKPTAYoHbtnCHImzbhsXYtn7z7LlfXrGFVXJy1qysRlFIkfPcde5cvx2/aNOqEhUk4Ebd39SqXLl1CHxaGi15v7WpECePo4ECb557jm2bNiJg8mdPFeAVkCSj2xsMDqlfHa/Nm5kVGcuKHH0iUSz0PRCnF1e+/J2r5cvymTqVew4boZJFGcQeWuDjOXb+OV2CgtUsRJZSjgwNNBw/mmKMj8bGx1i7nvslZ1B45OKDVqEHZJUvocOgQH331FQm3TOwm7p1SimNbt7Jv5UrKTphAvbAwdNJyUriUgmI8MZXp3Dn2N21KB19fa5ciSihN02jr60udqlVxKMazh0tAsWN6f38aT5lC/fXree+bb0jIzrZ2ScWKslg4sn07x95/n8ovvUSN8HC5rFMUduyA9u1h7do88/wUFze++ILL1aqBtLKJQqRpGjU6dGDf1q0SUETx5OjjQ4tWrWi4ZAlvrFsnISUfkjduJPbNN2kwZAgVZLRO0TCbYdEiCA+HIUPgp5/AYrF2VfcuM5PT3t483Lw5Rnm/iELmpGnUiYpixMGDPH/8ONE3b3K9GJ3jJaDYO03DcfBg2vTpQ5t583h940ZSZcXjf5WeksLBn38mvU8fQiScFB2LJWdOn6FD4dVX4bnnYP36YhNSTLt3c+XKFSr4+8t7RhQ6h3r1CPPw4KGtWzmfmUnL/ftpc+AAS2JjSTGZSDWbbbp1xWDtAoQN0OtxHDyY5gYD5gULuBQYSOX69eUEegdpaWlsXrAAF7OZ1l26oElTfdE5cgRKlwZ//5yQohQMHAgff5wzKaEtU4qs3bs5HBbGY25u1q5G2AO9HofGjekZHU3HF18kRdNYcukS2xITmXn+PC56PcPKlsVJp6O7jw/eDg7owGb60UlAETkMBtyffZYGzs4cnDYN/aRJVAwLs3ZVtufaNdKHDeO8pyfPzJyJu6entSuyL9u3Q1AQhIbm/H/YMDh1ChYuhJYtc+b+sVEKuLp1K9oLL2Abp39hF7p3h3btcHv7bdzc3JgRGkq62cz5zEzSzGbmxsSQYbGwJDaWTIuFp/z8CHN355FSpXC08pcvCSgil2YwENCnD+nOzhx74w20ceMIbdhQWlL+pK5fJ2noUD7w8CDglVdwKV3a2iUJgwFGjICePXNaVBYtyhlKb4uuXeNI6dI8VK+efKZE0XF2hlKlIDYWqlbNuUmvp5qLCwBf1qyJUooDqamcSE/n3YsX2ZiQwOzz50HTeKlsWcoZjVRydsa3iCcWlIAi8tD0eir06EG2Xs+hOXPImjCBqnXq2P0JNSspiTMTJrC+bFnKDB/OMyEhdv87sRmVK8O33+YsjjlsWM46VO7uXM/OJlsp/G1ktlbLli1EA2GurtYuRdiT8uWheXNYtSpn6ZPb0DSN+u7u1HNzo1eZMmQrxc8JCWQpxaLYWM6kp+Pj4EBlFxfGBAXhqtcTbDTiXMgTDUpAEf+gaRpVu3VDbzazZ84c0idPpl6VKnb7B9l04wYnXnuNsx4eDJg4EQ93d7v9XVhdy5Zw8CBkZ4ODw//fXqHC/4eUF1+E997jvWvXOJuezofVqlmv3r+YTKRv2EBqq1a0kMuCoijl41z113nNUdP4j48PSim6+vhwLTub/129SobFQq8jR7hpsfCQlxfeBgOPlS5NQ3d3vB0c0GsaZ9LTmXfhAk/6+tLKy+uBSpfefeK2NE2jYvfuNOnShe1vvcXWuDib7u1dWEw3bhA7ahRnXV156LXX8PLwsJkOZHapVi3YuTMnpNxK06BiRfj665xtI0ZgycrCbJ0q/0FZLFw8c4bQmjXl/SOKnsEAkZGQkpKvu2mahl7T8HN05IWyZRlZrhz7w8KIbtyYlp6eWJSiz7Fj1N+7l8lnzzLr/HnmxMSw9NIlDqelPXDZElDEHen0ekJbt+ax06fZPH8+m69csauQkpWUxPEJEzjq7c1DEyfi+YDfBkQBMZly5kO5nSpVckJKly45J2VbER1NVGAg9SpVsnYlwh6NGAG7d+d0Mn8AmqbhbjDgYTAwtGxZFlauzMGwMLbVr0+SycQvN27w/uXLeOr1VHF2fuCyJaCIu9L5+xOydCnP7d/PxnffJSafCby4ykxO5o9p07jk6EiLSZMknNgKTcsJIceP33mfatWga9d8NW0Xtuxff+VCYCDGAjhpC5FvoaHQpw98912BPqyDTkeQkxMVnZ15p3JlAHqWKcOuhg1pV6rUAz++BBTxr7QqVQh67z2e/eMPTq5YQUYJX1wwIyWFqFmzSDabCZ8yBfcC+KCJAmIwwDPPwEcfWbuSe2cykZKUBK1aUeXPkRNCFCkHh5xRbnv2wLlzhfIU6xMSOH7zJuODg6nm4lIg/fQkoIh7oq9QgYoLF1J62za2fPQRGSV0ccGMlBT2zpxJxs2bNJkyBVdpObFNGRlw86a1q7gnKiWFsxs30rxmTWuXIuxZcDBcvw4F0Dfk7zIsFt69eJF2pUpRrwAnIZSAIu6Jpmk4lC9Ptbffxnv7djasWEFmMVrT4V5kpKYS8cYbZKWm0mTyZFy9vWW0ji2qUweSk2Hs2GIxxb1lyxZ2VqxIiJ+ftUsR9szRMWeRzZ9+KvCH/uXGDfalpPBqcHCBnjMloIh7pmkazuXLU/vNN/H+9VfWrFpFRgkJKWlpafwyfz7q+nWaTpmCq4+PtUsSd1KlCsyfD//7X86IHRvvuH3zwAFMNWviYjRauxRhzxwcoFmzB+4o+3epZjPvxcbymI8PVQv4EqYEFJFvriEhNJg5E+/Nm/nim29IL+YhJSE7m7nr12PaupUmLVviUqaMtUsSd6Np0KoV9OqVM4PsgQM2G1JUairX4+MxhIdT+tZ5W4SwhtDQnEs8V64U2EPuTEpiR3IyrwQFoS/gFmcJKOK+uIWG0mz6dPw3b+az777jZjHtk3I9O5t569fTet48mvbvj3Pv3tYuSdwLZ2eYPTtnOHHv3pCQ8I9d9JpGBScnKxT3/yzXr3P+1Cm61Khh1TqEAKB165xwcuxYgTxcmtnMuxcv0tvXl8qFMEJNAoq4b64hITz08MO4f/wxVw4dsnY5+aaU4nRUFI0WLqTec8/h168fukKeulkUIGdnGD8+Z1bZFSv+sdmo0/FsQEDR13UL85Ur/FavHqVk9WJhC3Q6GDAgZ/XvAmh1jExOZldyMkPLlsWhEBYWlIAiHojzk08S/PTTHJo3jytHjxabidyUUlzYt4+U11+nda9eeA4ciGbllTvFfShTBoYMgUOHcoLKn5JNJtZfv27FwnJc/fRTgho0wFku7whboGk58wTdbR6he5RpsfBGTAy9fH2pVkjz+8gZWTwQnV5P0yefxL9jR/ZMm8aVkydtPqQopYiNimL/rFkEP/00Xs8+K6N1iitNg8GDYfPmPM3W6RYLfxTCcMr8UJcuEX/xIg716mGU8CtsRdWqoNfDkSP3/RBKKXYkJXEoLY3nAgIwFNL7Wz414oHp9Hoa9OmD76OPsmfSJGLPnrXZkPJXy8n+mTMJ6dePCj16oMllneLN2Rnq1oU337SpzrJZ165xVdPo8ucMm0LYhMqVcyY8jI6+74dQwOyYGDp5exfovCd/JwFFFAi9wUD9vn0JadOGyClTOH3xorVLuq3z+/bxx+uvU+G//6V2ly7oJZwUf05OMGwYbNkC+/ZZu5pc5qNHiahdG6MtrQkkBEDNmnD06H3PI/TLjRtkWCy8U7lyoS5+KQFFFBiDoyPVn32Weo0a8esbb/DH5cvWLimPM1FRHJ01i8p9+lC9WzfpEFuSPPJIzgRub71lM60oZzdtokWzZoXW/C3EfXv6afj00zsvunkXsZmZzDh/nqGBgbgV8jlUPjmiQOkdHQl94QU6VKzITwsXsscGOioqpTh94ADHZs+mSq9eVHniCQknJY1eD2PG5ExCFRlp7WpQBw4Qe+kSmZUqFeo3TCHui7MzpKbCzz/n625mpfjw8mVMStHe27uQivt/ElBEgdM5OFBu2DD6RUby9dKl7L5xw2p9UpRSnDp8mGPTp1PtiSeo2KuXdIgtqR5+GEaNgkWLCmW9kfy4mZhIfGAgVfz9rVqHELdVp07OtPeLFkF6+j3f7UpWFktiY3kxMBDvIhiZJgFFFArNwYGyc+bw4tq1fPjJJ0QlJxd5SFFKcez4cY5NmULNxx8ntGdPCSclmU6XM7NsVFRO87WVKKUwb9vG6bAwKhXS8EshHoim5bQ47t9/z4tuWpRiUWwsoU5OdCxdupALzCEBRRQOTUNr0ICQhQt55csveevrr9mbklJkIcWiFCeOHOHM+PGE/ec/lO/dWy7r2IPgYPjvf2H+fGorhYs1+n8oxYnDh+nerFnRP7cQ96pMGfD1hffeu6fdL2dlsTwujucCA/Eponl9JKCIwqNpaI0bU3n+fCatXMnra9awOzm50J82ITubaRs2cOHVV2nSqRMBTz+NTkZS2I9XXsG1VSsGR0fjao0Ws59+4nh6OkkBAdJiJ2yXnx889BCkpNzT7u9evIi/oyM9i3CtMgkootBpTZtS/a23mLpyJZPXr+dCRkahPdeN7Gze2riRBvPm0ahLF3yefRZNwol9MRpxGD4c3bp1ZBZBIP679PR0suvXp1apUkX+3ELky9NPw4YN/7p44LbERDYkJPBljRp4FuH5NF8BZfbs2TRq1Ah3d3d8fX3p1q0bx/82ZW5GRgZDhw6ldOnSuLm50aNHD+Lj4/PsExMTQ+fOnXFxccHX15cxY8ZgMpke/NUIm6U1bky9OXOYsGoV30dFkXGf4+/vJjE7m7kbN9J87lw6PfYYHkOGyCRsdsohOJiK8fHo72MY5YNQ2dkkrV3LpSZNiqQToRAPJDQULl+GzMw77pJuNjP57FkeKVWKikXcpypfAWXbtm0MHTqUXbt2sWnTJrKzs2nfvj1pt/SYHzVqFGvXrmX16tVs27aNS5cu8fjjj+duN5vNdO7cmaysLHbu3MnKlStZsWIFkydPLrhXJWyPpkGDBjTv2pWASZNYtH8/WQUYUjLMZmZs3UrzOXNo1707DsOHS/O6HdNZLLgWYkvdbVkssH49G7Ky6CL9T0QJoJRiS2Iip9PTGRAQgL6Iz6n5CigbNmzgmWeeoWbNmtStW5cVK1YQExNDVFQUAElJSXz00Ue8/fbbPPzwwzRs2JDly5ezc+dOdu3aBcDPP//M0aNH+eyzz6hXrx4dO3ZkxowZLF68mKysrIJ/hcJmaJqGrk8fHqtcGY/XX2fukSOYCqjT7OFLlwhbupRHunXDcdiwnBEdwm4pQFOKIjmdWiyoK1cwv/kmB2bM4K3HHsPi6FgUzyzEg4mIgLtcvbAAs8+fp1uZMtRwcSm6uv70QGfxpKQkALz/nLAlKiqK7Oxs2rVrl7tPtWrVCA4OJiIiAoCIiAhq166Nn59f7j4dOnQgOTmZI3dYvCgzM5Pk5OQ8P6J40pydcXj7bfr5+eEwdy7TTp4k+wFbUizp6Vjee4/Qxo1xGD48Z50JYdeUppFuNFJoY8aUQlksqHXrMM2fz8FevZh59iyfvPUWnz32GNWscDIXIl9SUmDJEnjySQgI+MdmpRTrrl8nJjOTQVbq8H3fAcVisTBy5EiaN29OrVq1AIiLi8PR0REvL688+/r5+REXF5e7z63h5K/tf227ndmzZ+Pp6Zn7ExQUdL9lC1vg6orzvHkMLlWKuNWrOXmP4/DvJDU9nZhjx6jUuzeaXPcXgEXTSHB3x1IYLWmZmahffuFGt26snTePQRkZLJ8+nS7z5jG3ZUvqubvL6sXC9u3YAXv3wogRt/1Sdy4jg2nnz7OwcmXqFuKCgHdz3181hw4dyuHDh/n9998Lsp7bGj9+PKNHj879f3JysoSU4s7NDc+RIxnZqxeTKldmRufO1HB1va+H0lksOCiFx9+CsbBjmkaqs3PBBpRTpzC//z6Xz59ne3Iyv/XoQctHH2WwhwdNPTykz5MoPm7ehIUL4aWXoEKFf2w2K8X7ly/jptPR2orn1fsKKMOGDePHH39k+/btlCtXLvd2f39/srKySExMzNOKEh8fj/+fUz77+/uze/fuPI/31ygf/ztMC200GjEajfdTqrBhWkgINWbNYsQbbzDK0ZH57dpR4z6SukpORmexYJE/EOIW6Ubjg78nkpJQv/xCyi+/cOLwYdaFhXG4e3c6P/QQ8/z9cdHpJJiI4ue333KGFo8fD7dpdb6YmcmHly+zNCgI16QkKKKZY/8uXwFFKcXw4cP57rvv+PXXXwkNDc2zvWHDhjg4OLB582Z69OgBwPHjx4mJiSE8PByA8PBwZs6cyZUrV/D19QVg06ZNeHh4UKNGjYJ4TaK40DS0du1ooWm8PnUqSy5f5pX+/QnJx/V7ZbFw5vPP0YeFocl1f3GLDEfH+wsoJhPcvIll/XoSP/uMr4xGfm7WjIovvsiAkBDGurpi1DQJJqJ4io6G4cNh1qw79tdbGhtLZWdnGuzezY49e2j15ptFXGSOfAWUoUOHsmrVKr7//nvc3d1z+4x4enri7OyMp6cnAwcOZPTo0Xh7e+Ph4cHw4cMJDw+nadOmALRv354aNWrQr18/5syZQ1xcHBMnTmTo0KHSSmKPNA1d27a4pabSePFiHFq2hD/7NN2L5JgYzkVFUXPKFIwyckL8SQEmvT5nePu9slhQp06hPvqIS2vX8k1YGCt696Zp3brMDA2lsrMzDtK3RBRnSuWsU+XpCf/5zx0/HxbgSFoak8+d46n4ePjqK+jVK3+fpwKQr4CydOlSAFq3bp3n9uXLl/PMM88AMH/+fHQ6HT169CAzM5MOHTqwZMmS3H31ej0//vgjL7zwAuHh4bi6utK/f3+mT5/+YK9EFFtmpThtMlErI4PAfH4AkoxGUkuVIrR69UKqThRXZp3u3kbxKIU6cgTLypXs27uXr6tWJXbePLrWr89qT09CnZ0xSGuJKAnOns0JKPPng9GYE1j+TtMYExTEwIAAXE0mAn/4Iae1pWdP2w4o97LQm5OTE4sXL2bx4sV33Kd8+fKsW7cuP08tSrCoiAh0X31FlTlz0GrWzNd9y5Qpg3uNGsR8+inln3kGnYziEYDJYMA5PR3HnTsxeXigU+q286IoION//yMyOpo1LVpwfvJketesyWulSlFK3kuipNmyBVxdwdkZvv8e3noLbp1tuVw5eOklygBlAgOhTZucCQhvGaRSlGTCCGFdiYlUGT+eyo88gtuf/ZTyw9lgILh3b36cM4dau3bRpmXLQihSFDfpLi5sevxxDkRHo1MKr5QUSqek4J6SQnJcHAFXr+KWns7FMmXY/fDDeA8YwMDQUGq5ukrfElGyRUfDCy9AUFBO8Lj1smVsLDz3XM6/vb1zVju+ejVnzR4rfC4koAjrcnTEc+BAuGU5hPyqW64c2TNmcOfVJIS9KeXgwMc9e+bMKAu5TdmZFgtfX7lCvMXCFXJaUF7y9qayszM6CSaipGveHJYuzel/EhCQE05ufd8rBUOG5Pz78GH4cwZ4OneWgCLskIsLWv/+D/QQmqbRSOZAEbfQNA193hsAcNHpeCYw0Co1CWF11avn/NyJpv3/yJ569XJ+rEi6pAshhBDC5khAEUIIIYTNkYAihBBCCJtTrPugJCUlkZCQYO0yRCFLSUnBZDJx48YNa5ciikBGRgaapsln206YzWZSU1PleNuB5OTke5qu5C+ays/eNiIpKQkvLy9Gjx6Nk5OTtcsRhUwphVIKncziaRcsFguAHG87YbFY0GTpALuglCIjI4P58+eTmJiIp6fnXfcvlgHlzJkzVKxY0dplCCGEEOI+XLhwIc9iw7dTLC/xeHt7AxATE/OvCUzYjuTkZIKCgrhw4QIeHh7WLkfcAzlmxZMct+LHXo6ZUoqUlBQC72G4f7EMKH81/Xp6epboA1lSeXh4yHErZuSYFU9y3Iofezhm99qwIBd5hRBCCGFzJKAIIYQQwuYUy4BiNBqZMmUKRqPR2qWIfJDjVvzIMSue5LgVP3LM/qlYjuIRQgghRMlWLFtQhBBCCFGySUARQgghhM2RgCKEEEIImyMBRQghhBA2RwKKEEIIIWxOsQwoixcvJiQkBCcnJ5o0acLu3butXZLdmj17No0aNcLd3R1fX1+6devG8ePH8+yTkZHB0KFDKV26NG5ubvTo0YP4+Pg8+8TExNC5c2dcXFzw9fVlzJgxmEymonwpduuNN95A0zRGjhyZe5scM9sUGxvLf//7X0qXLo2zszO1a9dm7969uduVUkyePJmAgACcnZ1p164dJ0+ezPMYCQkJ9O3bFw8PD7y8vBg4cCCpqalF/VLsgtlsZtKkSYSGhuLs7EzFihWZMWNGnhV95ZjdhSpmvvzyS+Xo6Kg+/vhjdeTIETVo0CDl5eWl4uPjrV2aXerQoYNavny5Onz4sDpw4IDq1KmTCg4OVqmpqbn7PP/88yooKEht3rxZ7d27VzVt2lQ1a9Ysd7vJZFK1atVS7dq1U/v371fr1q1TPj4+avz48dZ4SXZl9+7dKiQkRNWpU0eNGDEi93Y5ZrYnISFBlS9fXj3zzDMqMjJSnTlzRm3cuFGdOnUqd5833nhDeXp6qjVr1qiDBw+qxx57TIWGhqr09PTcfR599FFVt25dtWvXLvXbb7+pSpUqqaeeesoaL6nEmzlzpipdurT68ccf1dmzZ9Xq1auVm5ubeuedd3L3kWN2Z8UuoDRu3FgNHTo09/9ms1kFBgaq2bNnW7Eq8ZcrV64oQG3btk0ppVRiYqJycHBQq1evzt3n2LFjClARERFKKaXWrVundDqdiouLy91n6dKlysPDQ2VmZhbtC7AjKSkpqnLlymrTpk2qVatWuQFFjpltevXVV1WLFi3uuN1isSh/f381d+7c3NsSExOV0WhUX3zxhVJKqaNHjypA7dmzJ3ef9evXK03TVGxsbOEVb6c6d+6snn322Ty3Pf7446pv375KKTlm/6ZYXeLJysoiKiqKdu3a5d6m0+lo164dERERVqxM/CUpKQn4/xWno6KiyM7OznPMqlWrRnBwcO4xi4iIoHbt2vj5+eXu06FDB5KTkzly5EgRVm9fhg4dSufOnfMcG5BjZqt++OEHwsLC6NmzJ76+vtSvX59ly5blbj979ixxcXF5jpunpydNmjTJc9y8vLwICwvL3addu3bodDoiIyOL7sXYiWbNmrF582ZOnDgBwMGDB/n999/p2LEjIMfs3xSr1YyvXbuG2WzOc1IE8PPzIzo62kpVib9YLBZGjhxJ8+bNqVWrFgBxcXE4Ojri5eWVZ18/Pz/i4uJy97ndMf1rmyh4X375Jfv27WPPnj3/2CbHzDadOXOGpUuXMnr0aF577TX27NnDSy+9hKOjI/3798/9vd/uuNx63Hx9ffNsNxgMeHt7y3ErBOPGjSM5OZlq1aqh1+sxm83MnDmTvn37Asgx+xfFKqAI2zZ06FAOHz7M77//bu1SxF1cuHCBESNGsGnTJpycnKxdjrhHFouFsLAwZs2aBUD9+vU5fPgw7733Hv3797dydeJ2vv76az7//HNWrVpFzZo1OXDgACNHjiQwMFCO2T0oVpd4fHx80Ov1/xhNEB8fj7+/v5WqEgDDhg3jxx9/ZOvWrZQrVy73dn9/f7KyskhMTMyz/63HzN/f/7bH9K9tomBFRUVx5coVGjRogMFgwGAwsG3bNt59910MBgN+fn5yzGxQQEAANWrUyHNb9erViYmJAf7/936386O/vz9XrlzJs91kMpGQkCDHrRCMGTOGcePG0bt3b2rXrk2/fv0YNWoUs2fPBuSY/ZtiFVAcHR1p2LAhmzdvzr3NYrGwefNmwsPDrViZ/VJKMWzYML777ju2bNlCaGhonu0NGzbEwcEhzzE7fvw4MTExuccsPDycQ4cO5fkQbtq0CQ8Pj3+ckMWDa9u2LYcOHeLAgQO5P2FhYfTt2zf333LMbE/z5s3/MYT/xIkTlC9fHoDQ0FD8/f3zHLfk5GQiIyPzHLfExESioqJy99myZQsWi4UmTZoUwauwLzdv3kSny/tnVq/XY7FYADlm/8ravXTz68svv1RGo1GtWLFCHT16VA0ePFh5eXnlGU0gis4LL7ygPD091a+//qouX76c+3Pz5s3cfZ5//nkVHBystmzZovbu3avCw8NVeHh47va/hqy2b99eHThwQG3YsEGVKVNGhqwWoVtH8Sglx8wW7d69WxkMBjVz5kx18uRJ9fnnnysXFxf12Wef5e7zxhtvKC8vL/X999+rP/74Q3Xt2vW2Q1br16+vIiMj1e+//64qV65sF0NWraF///6qbNmyucOM//e//ykfHx81duzY3H3kmN1ZsQsoSim1cOFCFRwcrBwdHVXjxo3Vrl27rF2S3QJu+7N8+fLcfdLT09WLL76oSpUqpVxcXFT37t3V5cuX8zzOuXPnVMeOHZWzs7Py8fFRL7/8ssrOzi7iV2O//h5Q5JjZprVr16patWopo9GoqlWrpj744IM82y0Wi5o0aZLy8/NTRqNRtW3bVh0/fjzPPtevX1dPPfWUcnNzUx4eHmrAgAEqJSWlKF+G3UhOTlYjRoxQwcHBysnJSVWoUEFNmDAhz1B8OWZ3pil1y5R2QgghhBA2oFj1QRFCCCGEfZCAIoQQQgibIwFFCCGEEDZHAooQQgghbI4EFCGEEELYHAkoQgghhLA5ElCEEEIIYXMkoAghhBDC5khAEUIIIYTNkYAihBBCCJsjAUUIIYQQNuf/AJ446C8eJE03AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "same  different same  different\n"
     ]
    }
   ],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "\n",
    "#Sono invertiti le label rispetto alle immmagini?\n",
    "print(' '.join(f'{classes[labels[j]]:5s}' for j in range(parameters['train_batch_size'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "v1bPG_P5ZUKN"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lgiraldi/code/cabify/cabify_venv/lib/python3.10/site-packages/ignite/handlers/checkpoint.py:16: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.\n",
      "  from torch.distributed.optim import ZeroRedundancyOptimizer\n"
     ]
    }
   ],
   "source": [
    "from ignite.engine import Engine, Events\n",
    "from ignite.metrics import Loss, RunningAverage, Accuracy\n",
    "from ignite.metrics import ConfusionMatrix\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "import pathlib\n",
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "from ignite.handlers import Checkpoint, DiskSaver, global_step_from_engine\n",
    "import os\n",
    "\n",
    "\n",
    "def create_summary_writer(model, train_loader, log_dir, save_graph, device):\n",
    "    \"\"\"Creates a tensorboard summary writer\n",
    "\n",
    "    Arguments:\n",
    "        model {pytorch model}     -- the model whose graph needs to be saved\n",
    "        train_loader {dataloader} -- the training dataloader\n",
    "        log_dir {str}             -- the logging directory path\n",
    "        save_graph {bool}         -- if True a graph is saved into the\n",
    "                                     tensorboard log folder\n",
    "        device {torch.device}     -- torch device object\n",
    "\n",
    "    Returns:\n",
    "        writer -- tensorboard SummaryWriter object\n",
    "    \"\"\"\n",
    "    # writer = SummaryWriter(log_dir=log_dir)\n",
    "    # if save_graph:\n",
    "    #     images, labels = next(iter(train_loader))\n",
    "    #     images = images.to(device)\n",
    "    #     try:\n",
    "    #         writer.add_graph(model, images)\n",
    "    #     except Exception as e:\n",
    "    #         print(\"Failed to save model graph: {}\".format(e))\n",
    "    # return writer\n",
    "\n",
    "def train(model, optimizer, loss_fn, train_loader, val_loader,\n",
    "          log_dir, device, epochs, log_interval,\n",
    "          load_weight_path=None, save_graph=False):\n",
    "    \"\"\"Training logic for the wavelet model\n",
    "\n",
    "    Arguments:\n",
    "        model {pytorch model}       -- the model to be trained\n",
    "        optimizer {torch optim}     -- optimiser to be used\n",
    "        loss_fn                     -- loss_fn function\n",
    "        train_loader {dataloader}   -- training dataloader\n",
    "        val_loader {dataloader}     -- validation dataloader\n",
    "        log_dir {str}               -- the log directory\n",
    "        device {torch.device}       -- the device to be used e.g. cpu or cuda\n",
    "        epochs {int}                -- the number of epochs\n",
    "        log_interval {int}          -- the log interval for train batch loss\n",
    "\n",
    "    Keyword Arguments:\n",
    "        load_weight_path {str} -- Model weight path to be loaded (default: {None})\n",
    "        save_graph {bool}      -- whether to save the model graph (default: {False})\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    if load_weight_path is not None:\n",
    "        model.load_state_dict(torch.load(load_weight_path))\n",
    "\n",
    "    optimizer = optimizer(model.parameters())\n",
    "    accuracy = Accuracy()\n",
    "\n",
    "    #training steps\n",
    "    def process_function(engine, batch):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        x, labels = batch\n",
    "        x = x.to(device)\n",
    "        # print(x.size())\n",
    "        y = model(x)\n",
    "        labels = labels.to(device)\n",
    "        loss = loss_fn(y, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "    # evaluation steps (Val)\n",
    "    def evaluate_function(engine, batch):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            x, labels = batch\n",
    "            x = x.to(device)\n",
    "            y = model(x)\n",
    "            labels=labels.to(device)\n",
    "            loss = loss_fn(y,labels)\n",
    "            return loss.item()\n",
    "\n",
    "    # Questo oggetto contiene tutti i dati riguardanti il training\n",
    "    trainer = Engine(process_function)\n",
    "    # Questo oggetto contiene tutti i dati riguardanti la valutazione del modello. Ogni volta che viene chimato run su un determinato dataset\n",
    "    # il modello viene valutato.\n",
    "    # #Calcola la loss e segna ilnumero di epoche correnti, l'iterazione etc...\n",
    "    evaluator = Engine(evaluate_function)\n",
    "    accuracy.attach(evaluator, \"accuracy\") \n",
    "\n",
    "\n",
    "    RunningAverage(output_transform=lambda x:x).attach(trainer,'loss')\n",
    "    RunningAverage(output_transform=lambda x:x).attach(evaluator,'loss')\n",
    "    \n",
    "    # writer = create_summary_writer(model, train_loader, log_dir,\n",
    "    #                                save_graph, device)\n",
    "\n",
    "    def score_function(engine):\n",
    "        return -engine.state.metrics['loss']\n",
    "\n",
    "    to_save = {'model': model}\n",
    "    handler = Checkpoint(\n",
    "        to_save,\n",
    "        DiskSaver(os.path.join(log_dir, 'models'), create_dir=True),\n",
    "        n_saved=5, filename_prefix='best', score_function=score_function,\n",
    "        score_name=\"loss\",\n",
    "        global_step_transform=global_step_from_engine(trainer))\n",
    "\n",
    "    evaluator.add_event_handler(Events.COMPLETED, handler)\n",
    "\n",
    "    # Calcola la loss ogni \"log_interval\" iterazioni sul batch corrente. \"log interval\" di default vale 10.\n",
    "    @trainer.on(Events.ITERATION_COMPLETED(every=log_interval))\n",
    "    def log_training_loss(engine):\n",
    "        print(\n",
    "            f\"Epoch[{engine.state.epoch}] Iteration[{engine.state.iteration}/\"\n",
    "            f\"{len(train_loader)}] Loss: {engine.state.output:.10f}\"\n",
    "        )\n",
    "        # writer.add_scalar(\"training/loss\", engine.state.output,\n",
    "        #                   engine.state.iteration)\n",
    "\n",
    "    # Calcola la loss su tutto il training set una volta che l'epoca è finita\n",
    "    @trainer.on(Events.EPOCH_COMPLETED)\n",
    "    def log_training_results(engine):\n",
    "        evaluator.run(train_loader)\n",
    "        metrics = evaluator.state.metrics\n",
    "        avg_loss = metrics[\"loss\"]\n",
    "        print(\n",
    "            f\"Training Results - Epoch: {engine.state.epoch} Avg loss: {avg_loss:.10f}\"\n",
    "        )\n",
    "        # writer.add_scalar(\"training/avg_loss\", avg_loss, engine.state.epoch)\n",
    "\n",
    "    # Calcola la loss su tutto il validation set una volta che l'epoca è finita\n",
    "    @trainer.on(Events.EPOCH_COMPLETED)\n",
    "    def log_validation_results(engine):\n",
    "        evaluator.run(val_loader)\n",
    "        metrics = evaluator.state.metrics\n",
    "        avg_loss = metrics[\"loss\"]\n",
    "        avg_accuracy = metrics[\"accuracy\"] \n",
    "        print(\n",
    "            f\"Validation Results - Epoch: {engine.state.epoch}\"\n",
    "            f\"Avg loss: {avg_loss:.10f}, Avg accuracy: {avg_accuracy:.4f}\"\n",
    "        )\n",
    "        # writer.add_scalar(\"validation/avg_loss\", avg_loss, engine.state.epoch)\n",
    "\n",
    "    trainer.run(train_loader, max_epochs=epochs)\n",
    "\n",
    "    # writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u_XayXFLZUKN"
   },
   "source": [
    "trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 4155,
     "status": "ok",
     "timestamp": 1735650736073,
     "user": {
      "displayName": "lorenzo giraldi",
      "userId": "10228983639254068753"
     },
     "user_tz": -60
    },
    "id": "kzdNf0WjZUKN"
   },
   "outputs": [],
   "source": [
    "optimizer = Adam\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "loss = F.cross_entropy\n",
    "model=SimpleCNNconvNet224()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 938022,
     "status": "ok",
     "timestamp": 1735651676751,
     "user": {
      "displayName": "lorenzo giraldi",
      "userId": "10228983639254068753"
     },
     "user_tz": -60
    },
    "id": "naREcLCFZUKN",
    "outputId": "293079d8-82d2-4377-bdaf-0c548991c764"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "train(model, optimizer, loss, train_loader,\n",
    "          val_loader, parameters['log_dir'], device, parameters['epochs'],\n",
    "          parameters['log_interval'],\n",
    "          parameters['load_weight_path'], parameters['save_graph'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:tornado.general:SEND Error: Host unreachable\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m## First try on single batch ##\u001b[39;00m\n\u001b[1;32m      4\u001b[0m dataiter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(test_loader)\n\u001b[0;32m----> 5\u001b[0m images, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataiter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# print images\u001b[39;00m\n\u001b[1;32m      7\u001b[0m imshow(torchvision\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mmake_grid(images))\n",
      "File \u001b[0;32m~/code/cabify/cabify_venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/code/cabify/cabify_venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1448\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1448\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1449\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1451\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/code/cabify/cabify_venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1412\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1408\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1409\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1410\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1411\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1412\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1413\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1414\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/code/cabify/cabify_venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1243\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1230\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1231\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1232\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1240\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1241\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1242\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1243\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1244\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1245\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1246\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1247\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1248\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[1;32m    112\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py:424\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[0;32m--> 424\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py:931\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    928\u001b[0m     deadline \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 931\u001b[0m     ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[1;32m    933\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [key\u001b[38;5;241m.\u001b[39mfileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "File \u001b[0;32m/usr/lib/python3.10/selectors.py:416\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "PATH=\"tensorboard/logs_31122024_13-12/models/best_model_3_loss=-0.0815.pt\"\n",
    "\n",
    "## First try on single batch ##\n",
    "dataiter = iter(test_loader)\n",
    "images, labels = next(dataiter)\n",
    "# print images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print('GroundTruth: ', ' '.join(f'{classes[labels[j]]:5s}' for j in range(4)))\n",
    "\n",
    "net = SimpleCNNconvNet224()\n",
    "net.load_state_dict(torch.load(PATH, weights_only=True, map_location=torch.device('cpu')))\n",
    "outputs = net(images)\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "print('Predicted: ', ' '.join(f'{classes[predicted[j]]:5s}'\n",
    "                              for j in range(4)))  # Range 4 is corresponding to the batch size\n",
    "\n",
    "## On the hole test set##\n",
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the test set: {100 * correct // total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2671,
     "status": "ok",
     "timestamp": 1735652721881,
     "user": {
      "displayName": "lorenzo giraldi",
      "userId": "10228983639254068753"
     },
     "user_tz": -60
    },
    "id": "a7_EqdIcyEY6",
    "outputId": "1b458520-8706-4f58-bf73-0516cebf3fd7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: pathspec 'challenge.ipynb' did not match any files\n",
      "On branch master\n",
      "Your branch is up to date with 'origin/master'.\n",
      "\n",
      "Changes not staged for commit:\n",
      "  (use \"git add <file>...\" to update what will be committed)\n",
      "  (use \"git restore <file>...\" to discard changes in working directory)\n",
      "\t\u001b[31mmodified:   Challenge.ipynb\u001b[m\n",
      "\n",
      "Untracked files:\n",
      "  (use \"git add <file>...\" to include in what will be committed)\n",
      "\t\u001b[31mtensorboard/\u001b[m\n",
      "\n",
      "no changes added to commit (use \"git add\" and/or \"git commit -a\")\n",
      "Everything up-to-date\n"
     ]
    }
   ],
   "source": [
    "!git config --global user.email \"giraldilorenzo63@gmail.com\"\n",
    "!git config --global user.name \"lorenzoooooo\"\n",
    "\n",
    "!git add Challenge.ipynb\n",
    "!git commit -m \"Use of cross entropy loss and consequent update of process_function and evaluation_function\"\n",
    "!git push"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "cabify_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
