{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Challenge 1\n",
    "\n",
    "Success measure criterion --> Impact of service to increase restaurant's customers\n",
    "\n",
    "Impact measure criterion:\n",
    "    1. Compare sales before vs after picture posted\n",
    "    3. Compare quality of home made pictures with pro pictures -- The gap has decreased due to improvements and availability of technology and free tutorials\n",
    "    4. Since COVID more people order online --> Easier to sell food and then less need of high quality presentation? Or in the oder way higher standards?\n",
    "    5. Evolution of maintenance cost for photografy service\n",
    "\n",
    "Implementation:\n",
    "    Global:\n",
    "        For each year from 2001 to 2023: Number of subs to the service / Number of subscription to just eat --> Regression for the next X years\n",
    "        Evolution of the overall deployment and maintenance cost of the service --> regression for the next X years\n",
    "    Sample of restaurants divided by geographical criterion (maybe target, type of food):\n",
    "        Ratings (stars) of the service   \n",
    "        Percentage of sell increase with respect to before\n",
    "    Sample of customers divided by geographical criterion, target, type of food:\n",
    "        Percentage of orders coming from restaurant using photo service\n",
    "        Importance of picture criterion with respect to cost, qaulity and service criterion\n",
    "\n",
    "Avoid Bias by:\n",
    "    Create balanced dataset -- Balance by geographical areas, by price range\n",
    "    Normalization\n",
    "    K-Folding??\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Punto 2: Semplice RNN con dataset etichettato.\n",
    "\n",
    "Etichettatura multipla --> Softmax come attivazione finale\n",
    "\n",
    "0: “Both are the same”\n",
    "1: “They differ”\n",
    "2: “I don’t know”\n",
    "\n",
    "Come gestire il fatto che lo stesso percorso possa essere etichettato in maniera differente da 2 o + persone? Devo trovare un criterio per scegliere quale etichetta tenere. (Qual'è l'etichetta con la percentuale maggiore sullo stesso percorso?)\n",
    "\n",
    "Attenzione real e estimated routes no hanno per forza la stessa lunghezza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive', force_remount=True)\n",
    "# %cd /content/drive/My\\ Drive/cabify\n",
    "# # with open(\"github_token.txt\", \"r\") as f:\n",
    "# #   token = f.read()\n",
    "# #  ! git clone https://{token}@github.com/lorenzoooooo/cabify\n",
    "# %cd cabify\n",
    "# ! git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Load json to dataframe\n",
    "# df = pd.read_json('challenge_dataset.json')\n",
    "# print(f\"df size:  {df.size}\")\n",
    "\n",
    "# #Print a sample of real and estimated route\n",
    "# i=df.iloc[3]\n",
    "# df_estimated = pd.DataFrame(i['estimated_route'], columns=['Lat','Long'])\n",
    "# # print(df_estimated.size)\n",
    "# df_real = pd.DataFrame(i['real_route'], columns=['Lat','Long'])\n",
    "# # print(df_real.size)\n",
    "# plt.plot(df_estimated['Lat'], df_estimated['Long'], color='b')\n",
    "# plt.plot(df_real['Lat'], df_real['Long'], color='r')\n",
    "# # plt.scatter(df_estimated['Lat'], df_estimated['Long'], color='b')\n",
    "# # plt.scatter(df_real['Lat'], df_real['Long'], color='r')\n",
    "# plt.title('Annotator: ' + str(i['annotator'])+'\\n'+'Annotation: '+str(i['annotation']))\n",
    "# plt.legend(['Estimated route','Real route'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reflexions:\n",
    "    1. Dataset samples have different sizes -- Input size is variable --> how to declare NN?\n",
    "    2. Are points related temporally o spatially? RNN need equal temporal dependencies between samples. Otherwise can use Graph neural network.\n",
    "\n",
    "Approccio con feed forward:\n",
    "Layer iniziale deve avere dimensione ingresso -- Basic MLP are not fit to take variable input size, either introduce padding or cut to fized size\n",
    "\n",
    "GNN/GCN approach: Can take variable graph sizes as input\n",
    "\n",
    "The point is that I don't have single graph with label, but couple of graphs. I don't want to learn how to recognize the \"ight\" graph, I want to recognize if the graph differs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feed forward attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_size=??\n",
    "# number_of_features=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Net(nn.Module):\n",
    "\n",
    "#     def __init__(self):\n",
    "#         super(Net, self).__init__()\n",
    "\n",
    "#         # an affine operation: y = Wx + b\n",
    "#         self.fc1 = nn.Linear(input_size, number_of_features )  # 5*5 from image dimension\n",
    "#         self.fc2 = nn.Linear(120, 84)\n",
    "#         self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "#     def forward(self, input):\n",
    "#         f1 = F.relu(self.fc1(input))\n",
    "#         # Fully connected layer F6: (N, 120) Tensor input,\n",
    "#         # and outputs a (N, 84) Tensor, it uses RELU activation function\n",
    "#         f2 = F.relu(self.fc2(f1))\n",
    "#         # Gaussian layer OUTPUT: (N, 84) Tensor input, and\n",
    "#         # outputs a (N, 10) Tensor\n",
    "#         output = self.fc3(f2)\n",
    "#         return output\n",
    "\n",
    "\n",
    "# net = Net()\n",
    "# print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Possible alternative approach: Convolutional network --> Generate images of routes with associated labels and learn to recognize. 5000 images to generate -- Expensive approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate fixed size image dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Load dataset\n",
    "# df = pd.read_json('challenge_dataset.json')\n",
    "\n",
    "# # print(f'number of same routes annotations: {df[(df[\"annotation\"]==\"Both are the same\")].shape[0]}')\n",
    "# # print(f'number of different routes annotations: {df[(df[\"annotation\"]==\"They differ\")].shape[0]}')\n",
    "\n",
    "# #List containing final dataset composed of journey_id and annotation\n",
    "# rows_to_keep = []\n",
    "\n",
    "# #Check if same route has multiple rating and if the annotations are different choose the one who is more rated\n",
    "# iter = df['journey_id'].unique()\n",
    "# for i in iter: \n",
    "#     #Select all the rows regarding the same journey_id\n",
    "#     mask=(df['journey_id']==i)\n",
    "    \n",
    "#     #If this journey_id is rated only once append the journey_id and annotation to list\n",
    "#     if (df[mask].shape[0]==1):\n",
    "#         rows_to_keep.append([df[mask]['journey_id'].values[0], df[mask]['annotation'].values[0],\n",
    "#                              df[mask]['estimated_route'].values[0], df[mask]['real_route'].values[0]])\n",
    "    \n",
    "#     #Otherwise choose the more used annotation on this journey_id\n",
    "#     else:\n",
    "#         # Count the number of times an annotation appears\n",
    "#         x=df[mask]\n",
    "#         tmp=[]\n",
    "#         tmp.append(x[(x['annotation']==\"Both are the same\")].shape[0])\n",
    "#         tmp.append(x[(x['annotation']==\"They differ\")].shape[0])\n",
    "#         tmp.append(x[(x['annotation']==\"I don't know\")].shape[0])\n",
    "        \n",
    "#         #Select the annotation with max occurences\n",
    "#         # selected_label=max(tmp)\n",
    "#         idx_selected_label = tmp.index(max(tmp))\n",
    "        \n",
    "#         # Map the index to the corresponding label\n",
    "#         labels = [\"Both are the same\", \"They differ\", \"I don't know\"]\n",
    "#         selected_label_value = labels[idx_selected_label]\n",
    "        \n",
    "#         # Filter rows that match the selected label and append to list\n",
    "#         tmp_list=x[x['annotation'] == selected_label_value].iloc[0]\n",
    "#         rows_to_keep.append([tmp_list.values[0], tmp_list.values[2], \n",
    "#                              tmp_list.values[3], tmp_list.values[4]])\n",
    "\n",
    "# labels_path='labels.csv'\n",
    "# final_df=pd.DataFrame(rows_to_keep, columns=['journey_id', 'annotation', 'estimated_route', 'real_route'])\n",
    "# final_df.to_csv(labels_path, columns=['journey_id', 'annotation'], index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create directory to save images\n",
    "# output_dir = 'route_images'\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# # Turn interactive plotting off to not show every figure\n",
    "# plt.ioff()\n",
    "\n",
    "# #Create database of unique routes images \n",
    "# for _, df_sample in final_df.iterrows():      \n",
    "#     df_estimated = pd.DataFrame(df_sample['estimated_route'], columns=['Lat','Long'])\n",
    "#     df_real = pd.DataFrame(df_sample['real_route'], columns=['Lat','Long'])\n",
    "    \n",
    "#     fig = plt.figure(frameon=False)\n",
    "#     # Set the figure size to be square (for example, 6x6 inches)\n",
    "#     fig.set_size_inches(6, 6)\n",
    "    \n",
    "#     image_path = f'{output_dir}/{df_sample[\"journey_id\"]}.png'\n",
    "#     ax = plt.Axes(fig, [0., 0., 1., 1.])\n",
    "#     ax.set_axis_off()\n",
    "#     fig.add_axes(ax)\n",
    "\n",
    "#     plt.plot(df_estimated['Lat'], df_estimated['Long'], color='c')\n",
    "#     plt.plot(df_real['Lat'], df_real['Long'], color='r')\n",
    "#     # plt.savefig(image_path, dpi=300, bbox_inches='tight') # Dimension = 1920*1440 pixels\n",
    "#     plt.savefig(image_path, dpi=37, bbox_inches='tight')   # Dimension 224*224\n",
    "#     # plt.savefig(image_path, dpi=5, bbox_inches='tight')   # Dimension 32*32\n",
    "#     plt.close() \n",
    "\n",
    "# #Execution time - 6min39s for image Dimension = 1920*1440 pixels\n",
    "# #Execution time - 1min30s for image Dimension = 224*224 pixels\n",
    "# #Execution time - 31s for image Dimension = 32*32 pixels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataset splitting in Train, Val and Test - Unbalanced\n",
    "----Run only once, oherwise it adds to the dataset previously created ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "source_dir = 'route_images'\n",
    "dataset_dir='dataset'\n",
    "val_dir = f'{dataset_dir}/Val/'\n",
    "train_dir = f'{dataset_dir}/Train/'\n",
    "test_dir = f'{dataset_dir}/Test/'\n",
    "output_dir = 'labels.csv'\n",
    "# labels_list=['same','different','idk']\n",
    "#For now i don't create the idk class\n",
    "# labels_list=['same','different']\n",
    "labels_list=['different','same']\n",
    "dir_list=[train_dir, test_dir, val_dir]\n",
    "\n",
    "# Do the following only if the dataset has not been already created. To create new just empty the dataset folder\n",
    "if len(os.listdir(dataset_dir)) == 0:\n",
    "    #### Split Dataset equally between Train and Validation #############################################################\n",
    "    for i in dir_list:\n",
    "        if not os.path.exists(i):\n",
    "            os.makedirs(i)\n",
    "        for j in labels_list:\n",
    "            path=f'{i}{j}'\n",
    "            print(path)\n",
    "            if not os.path.exists(path):\n",
    "                os.makedirs(path)\n",
    "\n",
    "    ## Import csv file containing labels and annotation and count the number of total elements\n",
    "    labels = pd.read_csv(output_dir)\n",
    "    num_files = len(labels)\n",
    "    print('Numero di files nella cartella: ', num_files)\n",
    "\n",
    "\n",
    "    # Create mask for each annotation\n",
    "    mask_same = (labels['annotation']=='Both are the same')\n",
    "    print(f'number of same elements: {len(labels[mask_same])}')\n",
    "    mask_different = (labels['annotation']=='They differ')\n",
    "    print(f'number of diff elements: {len(labels[mask_different])}')\n",
    "    mask_idk=(labels['annotation']==\"I don't know\")\n",
    "    print(f'number of idk elements: {len(labels[mask_idk])}')\n",
    "\n",
    "    #Divide between training, validation and test (70%,10%,20%) --- Not using label I don't know\n",
    "    #Create Training set\n",
    "    train_set=[]\n",
    "    x=labels[mask_same]['journey_id'].to_list()\n",
    "    y=labels[mask_different]['journey_id'].to_list()\n",
    "    train_set = random.sample(x, int(0.70*len(x))) + random.sample(y, int(0.70*len(y)))\n",
    "\n",
    "    #Select the remaining data and split it between test (2/3) and validation set (1/3)\n",
    "    # To apply the difference operator I hve to cast the data to set. I cast it back to list for random samples.\n",
    "    remaining_x_set = set(x).difference(train_set)\n",
    "    remaining_y_set = set(y).difference(train_set)\n",
    "    test_set = (random.sample(list(remaining_x_set), int(0.66*len(remaining_x_set))) + \n",
    "                random.sample(list(remaining_y_set), int(0.66*len(remaining_y_set))))\n",
    "    val_set = set(list(remaining_x_set) + list(remaining_y_set)).difference(test_set)\n",
    "\n",
    "    #Copy images in the respective folder\n",
    "    for i in labels.values:\n",
    "        idx=i[1]\n",
    "        files=i[0]\n",
    "    \n",
    "        #Categorize data based on the labels\n",
    "        if idx == 'Both are the same':\n",
    "            folder=labels_list[0]\n",
    "        elif idx == 'They differ':\n",
    "            folder=labels_list[1]\n",
    "        # else:\n",
    "        #     folder=labels_list[2]\n",
    "\n",
    "        #Place data in the correct dataset spit (Train, Test, Validation)    \n",
    "        if files in train_set:\n",
    "            shutil.copy(os.path.join(source_dir, files+'.png'), f'{train_dir}{folder}')\n",
    "        elif files in test_set:\n",
    "            shutil.copy(os.path.join(source_dir, files+'.png'), f'{test_dir}{folder}')\n",
    "        elif files in val_set:\n",
    "            shutil.copy(os.path.join(source_dir, files+'.png'), f'{val_dir}{folder}')\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "size: 32*32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class convNet32(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "#         self.pool = nn.MaxPool2d(2, 2)\n",
    "#         self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "\n",
    "#         # self.fc1 = nn.Linear(16 * 5 * 5, 120)  ## From the calculation on papaer it should be input dimension=16*10*10\n",
    "#         self.fc1 = nn.Linear(16 * 10 * 10, 120)  \n",
    "#         self.fc2 = nn.Linear(120, 84)\n",
    "#         self.fc3 = nn.Linear(84, 10)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.pool(F.relu(self.conv1(x)))\n",
    "#         x = self.pool(F.relu(self.conv2(x)))\n",
    "#         x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = F.relu(self.fc2(x))\n",
    "#         x = self.fc3(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "size 224*224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleCNNconvNet224(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=200704, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (fc3): Linear(in_features=1024, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class SimpleCNNconvNet224(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNNconvNet224, self).__init__()\n",
    "        \n",
    "        # First convolution layer\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)  # Output: 224x224x64\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)      # Output: 112x112x64\n",
    "        \n",
    "        # Second convolution layer\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)  # Output: 112x112x128\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)       # Output: 56x56x128\n",
    "        \n",
    "        # Third convolution layer\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1) # Output: 56x56x256\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)       # Output: 28x28x256\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(256 * 28 * 28, 4096)   # Flattened input to FC layer\n",
    "        self.fc2 = nn.Linear(4096, 1024)             # Second FC layer\n",
    "        self.fc3 = nn.Linear(1024, 2)               # Output layer (2 classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass through first convolution and max pool\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        print(f'dimensione di x fopo conv1: {x.size()}')\n",
    "        # Pass through second convolution and max pool\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        print(f'dimensione di x fopo conv2: {x.size()}')\n",
    "        # Pass through third convolution and max pool\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool3(x)\n",
    "        print(f'dimensione di x fopo conv3: {x.size()}')\n",
    "        # Flatten the output for the fully connected layers\n",
    "        # x = x.view(-1, 256 * 28 * 28)\n",
    "        x=torch.flatten(x,1)\n",
    "\n",
    "        # Pass through fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        print(f'dimensione di x fopo fc1: {x.size()}')\n",
    "        x = F.relu(self.fc2(x))\n",
    "        print(f'dimensione di x fopo fc2: {x.size()}')\n",
    "        x = self.fc3(x)\n",
    "        print(f'dimensione di x fopo fc3: {x.size()}')\n",
    "        return x\n",
    "\n",
    "print(SimpleCNNconvNet224())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delcare hyperparameters, choose optimizer, determine if it's possible to use cpu or gpu, and load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/Train/\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1:\n",
      "Images shape:  torch.Size([10, 3, 224, 224])\n",
      "Batch 2:\n",
      "Images shape:  torch.Size([10, 3, 224, 224])\n",
      "Batch 3:\n",
      "Images shape:  torch.Size([10, 3, 224, 224])\n",
      "dataset/Val/\n",
      "\n",
      "Batch 1:\n",
      "Images shape:  torch.Size([10, 3, 224, 224])\n",
      "Batch 2:\n",
      "Images shape:  torch.Size([10, 3, 224, 224])\n",
      "Batch 3:\n",
      "Images shape:  torch.Size([10, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import Compose, ToTensor, Grayscale, CenterCrop\n",
    "import argparse\n",
    "import torch.nn.functional as F\n",
    "import torch.nn\n",
    "from torch.optim import Adam\n",
    "import torch\n",
    "from datetime import datetime\n",
    "\n",
    "# Function to transform image\n",
    "def create_datagen(data_dir, batch_size=8):\n",
    "    # transform = Compose([Grayscale(), ToTensor()])    # for bn images\n",
    "    transform = Compose([ToTensor(), CenterCrop([224,224]),\n",
    "                         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])   # input: [224*224]\n",
    "    dataset = ImageFolder(data_dir, transform=transform)\n",
    "    dataloader = DataLoader(dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=True,\n",
    "                            num_workers=4)\n",
    "    return dataloader\n",
    "    \n",
    "def dataset_description(data_loader):\n",
    "    print(data_loader.dataset.root) \n",
    "    print()  # Add an empty line for better readability\n",
    "    for batch_idx, (images, _) in enumerate(data_loader):\n",
    "        print(f\"Batch {batch_idx+1}:\")\n",
    "        print(\"Images shape: \", images.size())  # Prints shape of the batch of images\n",
    "        \n",
    "        # Optionally, you can break after printing a few batches\n",
    "        if batch_idx == 2:\n",
    "            break\n",
    "\n",
    "parameters = {'train_dir': train_dir,\n",
    "                'val_dir': val_dir,\n",
    "                'log_interval': 10,\n",
    "                'epochs': 100,\n",
    "                'train_batch_size': 10,\n",
    "                'val_batch_size': 10,\n",
    "                'log_dir': f'tensorboard/logs_{datetime.now().strftime(\"%d%m%Y_%H-%M\")}',\n",
    "                'save_graph': True,\n",
    "                'load_weight_path': None\n",
    "                }\n",
    "\n",
    "train_loader = create_datagen(parameters['train_dir'], parameters['train_batch_size'])\n",
    "dataset_description(train_loader)\n",
    "val_loader = create_datagen(parameters['val_dir'], parameters['val_batch_size'])\n",
    "dataset_description(val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show single mini-batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3, 224, 224])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "same  same  different same  same  different different different different different\n"
     ]
    }
   ],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "classes=tuple(labels_list)\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "\n",
    "#Sono invertiti le label rispetto alle immmagini?\n",
    "print(' '.join(f'{classes[labels[j]]:5s}' for j in range(parameters['train_batch_size'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lgiraldi/code/cabify/cabify_venv/lib/python3.10/site-packages/ignite/handlers/checkpoint.py:16: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.\n",
      "  from torch.distributed.optim import ZeroRedundancyOptimizer\n"
     ]
    }
   ],
   "source": [
    "from ignite.engine import Engine, Events\n",
    "from ignite.metrics import Loss, RunningAverage\n",
    "from ignite.metrics import ConfusionMatrix\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "import pathlib\n",
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "from ignite.handlers import Checkpoint, DiskSaver, global_step_from_engine\n",
    "import os\n",
    "\n",
    "\n",
    "def create_summary_writer(model, train_loader, log_dir, save_graph, device):\n",
    "    \"\"\"Creates a tensorboard summary writer\n",
    "\n",
    "    Arguments:\n",
    "        model {pytorch model}     -- the model whose graph needs to be saved\n",
    "        train_loader {dataloader} -- the training dataloader\n",
    "        log_dir {str}             -- the logging directory path\n",
    "        save_graph {bool}         -- if True a graph is saved into the\n",
    "                                     tensorboard log folder\n",
    "        device {torch.device}     -- torch device object\n",
    "\n",
    "    Returns:\n",
    "        writer -- tensorboard SummaryWriter object\n",
    "    \"\"\"\n",
    "    # writer = SummaryWriter(log_dir=log_dir)\n",
    "    # if save_graph:\n",
    "    #     images, labels = next(iter(train_loader))\n",
    "    #     images = images.to(device)\n",
    "    #     try:\n",
    "    #         writer.add_graph(model, images)\n",
    "    #     except Exception as e:\n",
    "    #         print(\"Failed to save model graph: {}\".format(e))\n",
    "    # return writer\n",
    "\n",
    "def train(model, optimizer, loss_fn, train_loader, val_loader,\n",
    "          log_dir, device, epochs, log_interval,\n",
    "          load_weight_path=None, save_graph=False):\n",
    "    \"\"\"Training logic for the wavelet model\n",
    "\n",
    "    Arguments:\n",
    "        model {pytorch model}       -- the model to be trained\n",
    "        optimizer {torch optim}     -- optimiser to be used\n",
    "        loss_fn                     -- loss_fn function\n",
    "        train_loader {dataloader}   -- training dataloader\n",
    "        val_loader {dataloader}     -- validation dataloader\n",
    "        log_dir {str}               -- the log directory\n",
    "        device {torch.device}       -- the device to be used e.g. cpu or cuda\n",
    "        epochs {int}                -- the number of epochs\n",
    "        log_interval {int}          -- the log interval for train batch loss\n",
    "\n",
    "    Keyword Arguments:\n",
    "        load_weight_path {str} -- Model weight path to be loaded (default: {None})\n",
    "        save_graph {bool}      -- whether to save the model graph (default: {False})\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    if load_weight_path is not None:\n",
    "        model.load_state_dict(torch.load(load_weight_path))\n",
    "\n",
    "    optimizer = optimizer(model.parameters())\n",
    "\n",
    "    def process_function(engine, batch):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        x, _ = batch\n",
    "        x = x.to(device)\n",
    "        print(x.size())\n",
    "        y = model(x)\n",
    "        loss = loss_fn(y, x)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "    def evaluate_function(engine, batch):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            x, _ = batch\n",
    "            x = x.to(device)\n",
    "            y = model(x)\n",
    "            loss = loss_fn(y,x)\n",
    "            return loss.item()\n",
    "\n",
    "    # Questo oggetto contiene tutti i dati riguardanti il training\n",
    "    trainer = Engine(process_function)\n",
    "    # Questo oggetto contiene tutti i dati riguardanti la valutazione del modello. Ogni volta che viene chimato run su un determinato dataset \n",
    "    # il modello viene valutato. \n",
    "    # #Calcola la loss e segna ilnumero di epoche correnti, l'iterazione etc...\n",
    "    evaluator = Engine(evaluate_function)\n",
    "\n",
    "    RunningAverage(output_transform=lambda x:x).attach(trainer,'loss')\n",
    "    RunningAverage(output_transform=lambda x:x).attach(evaluator,'loss')\n",
    "\n",
    "\n",
    "    # writer = create_summary_writer(model, train_loader, log_dir,\n",
    "    #                                save_graph, device)\n",
    "\n",
    "    def score_function(engine):\n",
    "        return -engine.state.metrics['loss']\n",
    "\n",
    "    to_save = {'model': model}\n",
    "    handler = Checkpoint(\n",
    "        to_save,\n",
    "        DiskSaver(os.path.join(log_dir, 'models'), create_dir=True),\n",
    "        n_saved=5, filename_prefix='best', score_function=score_function,\n",
    "        score_name=\"loss\",\n",
    "        global_step_transform=global_step_from_engine(trainer))\n",
    "\n",
    "    evaluator.add_event_handler(Events.COMPLETED, handler)\n",
    "    \n",
    "    # Calcola la loss ogni \"log_interval\" iterazioni sul batch corrente. \"log interval\" di default vale 10.\n",
    "    @trainer.on(Events.ITERATION_COMPLETED(every=log_interval))\n",
    "    def log_training_loss(engine):\n",
    "        print(\n",
    "            f\"Epoch[{engine.state.epoch}] Iteration[{engine.state.iteration}/\"\n",
    "            f\"{len(train_loader)}] Loss: {engine.state.output:.10f}\"\n",
    "        )\n",
    "        # writer.add_scalar(\"training/loss\", engine.state.output,\n",
    "        #                   engine.state.iteration)\n",
    "\n",
    "    # Calcola la loss su tutto il training set una volta che l'epoca è finita\n",
    "    @trainer.on(Events.EPOCH_COMPLETED)\n",
    "    def log_training_results(engine):\n",
    "        evaluator.run(train_loader)\n",
    "        metrics = evaluator.state.metrics\n",
    "        avg_loss = metrics[\"loss\"]\n",
    "        print(\n",
    "            f\"Training Results - Epoch: {engine.state.epoch} Avg loss: {avg_loss:.10f}\"\n",
    "        )\n",
    "        # writer.add_scalar(\"training/avg_loss\", avg_loss, engine.state.epoch)\n",
    "\n",
    "    # Calcola la loss su tutto il validation set una volta che l'epoca è finita\n",
    "    @trainer.on(Events.EPOCH_COMPLETED)\n",
    "    def log_validation_results(engine):\n",
    "        evaluator.run(val_loader)\n",
    "        metrics = evaluator.state.metrics\n",
    "        avg_loss = metrics[\"loss\"]\n",
    "\n",
    "        print(\n",
    "            f\"Validation Results - Epoch: {engine.state.epoch} Avg loss: {avg_loss:.10f}\"\n",
    "        )\n",
    "        # writer.add_scalar(\"validation/avg_loss\", avg_loss, engine.state.epoch)\n",
    "\n",
    "    trainer.run(train_loader, max_epochs=epochs)\n",
    "\n",
    "    # writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "loss = F.mse_loss\n",
    "model=SimpleCNNconvNet224()\n",
    "\n",
    "# model.to(device)\n",
    "# optimizer = optimizer(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3, 224, 224])\n",
      "dimensione di x fopo conv1: torch.Size([10, 64, 112, 112])\n",
      "dimensione di x fopo conv2: torch.Size([10, 128, 56, 56])\n",
      "dimensione di x fopo conv3: torch.Size([10, 256, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_148793/2000460018.py:72: UserWarning: Using a target size (torch.Size([10, 3, 224, 224])) that is different to the input size (torch.Size([10, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = loss_fn(y, x)\n",
      "Current run is terminating due to exception: The size of tensor a (2) must match the size of tensor b (224) at non-singleton dimension 3\n",
      "Engine run is terminating due to exception: The size of tensor a (2) must match the size of tensor b (224) at non-singleton dimension 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimensione di x fopo fc1: torch.Size([10, 4096])\n",
      "dimensione di x fopo fc2: torch.Size([10, 1024])\n",
      "dimensione di x fopo fc3: torch.Size([10, 2])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (224) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m          \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlog_dir\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepochs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m          \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlog_interval\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m          \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mload_weight_path\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msave_graph\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[22], line 146\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, loss_fn, train_loader, val_loader, log_dir, device, epochs, log_interval, load_weight_path, save_graph)\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation Results - Epoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mengine\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Avg loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.10f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    143\u001b[0m     )\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;66;03m# writer.add_scalar(\"validation/avg_loss\", avg_loss, engine.state.epoch)\u001b[39;00m\n\u001b[0;32m--> 146\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/cabify/cabify_venv/lib/python3.10/site-packages/ignite/engine/engine.py:889\u001b[0m, in \u001b[0;36mEngine.run\u001b[0;34m(self, data, max_epochs, epoch_length)\u001b[0m\n\u001b[1;32m    886\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mdataloader \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m    888\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterrupt_resume_enabled:\n\u001b[0;32m--> 889\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_internal_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    891\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_run_legacy()\n",
      "File \u001b[0;32m~/code/cabify/cabify_venv/lib/python3.10/site-packages/ignite/engine/engine.py:932\u001b[0m, in \u001b[0;36mEngine._internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    930\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_run_generator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_run_as_gen()\n\u001b[1;32m    931\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 932\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_internal_run_generator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    933\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m out:\n\u001b[1;32m    934\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_run_generator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/code/cabify/cabify_venv/lib/python3.10/site-packages/ignite/engine/engine.py:990\u001b[0m, in \u001b[0;36mEngine._internal_run_as_gen\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    988\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataloader_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    989\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine run is terminating due to exception: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 990\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    992\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataloader_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    993\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\n",
      "File \u001b[0;32m~/code/cabify/cabify_venv/lib/python3.10/site-packages/ignite/engine/engine.py:644\u001b[0m, in \u001b[0;36mEngine._handle_exception\u001b[0;34m(self, e)\u001b[0m\n\u001b[1;32m    642\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fire_event(Events\u001b[38;5;241m.\u001b[39mEXCEPTION_RAISED, e)\n\u001b[1;32m    643\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 644\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/code/cabify/cabify_venv/lib/python3.10/site-packages/ignite/engine/engine.py:956\u001b[0m, in \u001b[0;36mEngine._internal_run_as_gen\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    953\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataloader_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    954\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setup_engine()\n\u001b[0;32m--> 956\u001b[0m epoch_time_taken \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_once_on_dataset_as_gen()\n\u001b[1;32m    958\u001b[0m \u001b[38;5;66;03m# time is available for handlers but must be updated after fire\u001b[39;00m\n\u001b[1;32m    959\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mtimes[Events\u001b[38;5;241m.\u001b[39mEPOCH_COMPLETED\u001b[38;5;241m.\u001b[39mname] \u001b[38;5;241m=\u001b[39m epoch_time_taken\n",
      "File \u001b[0;32m~/code/cabify/cabify_venv/lib/python3.10/site-packages/ignite/engine/engine.py:1096\u001b[0m, in \u001b[0;36mEngine._run_once_on_dataset_as_gen\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1094\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1095\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrent run is terminating due to exception: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1096\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/code/cabify/cabify_venv/lib/python3.10/site-packages/ignite/engine/engine.py:644\u001b[0m, in \u001b[0;36mEngine._handle_exception\u001b[0;34m(self, e)\u001b[0m\n\u001b[1;32m    642\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fire_event(Events\u001b[38;5;241m.\u001b[39mEXCEPTION_RAISED, e)\n\u001b[1;32m    643\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 644\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/code/cabify/cabify_venv/lib/python3.10/site-packages/ignite/engine/engine.py:1077\u001b[0m, in \u001b[0;36mEngine._run_once_on_dataset_as_gen\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1074\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fire_event(Events\u001b[38;5;241m.\u001b[39mITERATION_STARTED)\n\u001b[1;32m   1075\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_terminate_or_interrupt()\n\u001b[0;32m-> 1077\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39moutput \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1078\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fire_event(Events\u001b[38;5;241m.\u001b[39mITERATION_COMPLETED)\n\u001b[1;32m   1079\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_terminate_or_interrupt()\n",
      "Cell \u001b[0;32mIn[22], line 72\u001b[0m, in \u001b[0;36mtrain.<locals>.process_function\u001b[0;34m(engine, batch)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28mprint\u001b[39m(x\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m     71\u001b[0m y \u001b[38;5;241m=\u001b[39m model(x)\n\u001b[0;32m---> 72\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     74\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/code/cabify/cabify_venv/lib/python3.10/site-packages/torch/nn/functional.py:3791\u001b[0m, in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3789\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3791\u001b[0m expanded_input, expanded_target \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3792\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mmse_loss(\n\u001b[1;32m   3793\u001b[0m     expanded_input, expanded_target, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction)\n\u001b[1;32m   3794\u001b[0m )\n",
      "File \u001b[0;32m~/code/cabify/cabify_venv/lib/python3.10/site-packages/torch/functional.py:76\u001b[0m, in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(tensors):\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, \u001b[38;5;241m*\u001b[39mtensors)\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (224) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, loss, train_loader,\n",
    "          val_loader, parameters['log_dir'], device, parameters['epochs'],\n",
    "          parameters['log_interval'],\n",
    "          parameters['load_weight_path'], parameters['save_graph'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cabify_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
